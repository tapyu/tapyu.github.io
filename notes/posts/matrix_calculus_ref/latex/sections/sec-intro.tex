\chapter{Introduction}
Since my Master's degree, I've been struggling with matrix differentiation as I could not find good references that cover it nicely. The bibliographies I found at that time were books from Economics \cite{dhrymes1978mathematics}, but they use an \st{weird} unfamiliar notation.

After delving a lot, I finally found a good reference from Professor Randal's class note \cite{barnes2006matrix}. However, to my surprise, when I tried to apply those matrix differentiation propositions, I got ``wrong'' answers! The truth is that \emph{Matrix Calculus notation is severely fragmented and there is no consensus among the researchers over which notation to follow}. Fortunately, there are only two major ways to represent a derivative of a vector \cite{Singh}. If you do not select the author's representation, you will end up with the transposed result. The first representation is called Jacobian formulation or numerator layout, while the second one is called Hessian formulation or denominator layout. Nevertheless, even for the same layout, some conventions need to be stated.

Due to the lack of references and the need to have one, I decided to make this guide. The goal here is twofold: introduce the theoretical aspects of Matrix Calculus (first part) and solve partial derivatives for the most common Matrix Calculus expressions you may come across (second part). The second part is nothing more than a manual solution for some expressions, which might be useful to the reader as a quick reminder of how to solve them. The step-by-step is given. Most problems will be defined in real space, but some will be extended to complex space. Furthermore, \textbf{I will adopt only the demodulator layout} since it matches the notation commonly used by authors in the field of Signal Processing, Machine Learning, and Optimization Theory. On the other hand, the first part might be valuable for those who are trying to understand theoretical concepts, conventions, notations, identities, and applications\footnote{Especially to Optimization Theory.} of Matrix Calculus.

\section{Curated reference list}
The following list shows some references you can rely on besides this guide when it comes to Matrix Calculus (in decreasing order of importance):
\begin{itemize}
    \item Searle, Shayle R., and Andre I. Khuri. Matrix algebra useful for statistics \cite{searle2017matrix}: A classical book with the first edition in 1987. It treats Matrix Calculus (in denominator layout) in chapter 9 of the second edition.
    \item Hj\o rungnes, Are, and David Gesbert. Complex-valued matrix differentiation: Techniques and key results \cite{hjorungnes2011complex}. An advanced book that uses Wirtinger calculus to differentiate complex-valued matrices. You might prefer his article \cite{hjorungnes2007complex} which summarizes the key results.
    \item You can find good references in the following book appendices: Dattoro \cite{dattorroConvexOptimizationEuclidean2010}, appendix D; Bishop \cite{bishopPatternRecognitionMachine2006}, appendix C; Simon Haykin \cite{haykin2009neural}, appendix B (Wirtinger calculus).
    \item Old and New Matrix Algebra Useful for Statistics, Thomas Minka \cite{ThomasMinka}: Another good guide for matrix algebra hosted on the personal site of the author. It is very well-referenced and focuses more on the concepts of Matrix Calculus than solving the derivatives.
    \item The Matrix Cookbook \cite{petersen2008matrix}: Good reference to see Matrix Calculus expression results rather than understanding how they were solved.
    \item Lecture notes in Introduction to Machine Learning, from Carnegie Mellon University \cite{Singh}: A short lecture but with a clear explanation of the difference between numerator and denominator layout.
    \item Matrix Calculus You Need For Deep Learning \cite{parrMatrixCalculusYou2018}: Preprint article focused on its application on Machine Learning. It offers the best tradeoff between comprehension and brevity as it is just 33 pages long. It uses the numerator layout, though.
    \item Wikipedia \cite{Matrixca44:online}: Despite not being a good to base on, it is surprisingly comprehensive.
    \item Professor Randal's class note \cite{barnes2006matrix}: It has the same purpose as this guide, that is, to solve common matrix differentiation, but it is done using the numerator layout. Such a layout is rarely used in the references herein cited.
\end{itemize}

Differentiation solutions that were collected from other sources will be referenced, while solutions that I derived by myself will not have any reference. Obviously, this guide may have errors (I hope not). If you find it, feel free to reach out through email or simply make a pull request on my \href{https://github.com/tapyu/tapyu.github.io/tree/master/notes/posts/matrix_diff_ref/latex}{Github}.