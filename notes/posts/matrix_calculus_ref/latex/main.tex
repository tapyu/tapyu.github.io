\documentclass{article}

% redefine \maketitle
\makeatletter
\def\@maketitle{%
  \newpage
  \null
  \vskip 2em%
  \begin{center}%
  \let \footnote \thanks
    {\LARGE \@title \par}%
    \vskip 1.5em%
    {\large
      \lineskip .5em%
      \begin{tabular}[t]{c}%
        \@author\\
      \end{tabular}\par}%
    \vskip 1em%
    {\large {\tt Version:}\@date}%
  \end{center}%
  \par
  \vskip 1.5em}
\makeatother

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}
\usepackage{float}
\usepackage{multirow}

% math
\usepackage{amsfonts}
\newcommand{\trans}{\top}
\newcommand{\hermit}{\mathsf{H}}
\newcommand{\tr}[1]{\ensuremath{\textnormal{tr}\left(#1\right)}} % trace
\newcommand{\adj}[1]{\ensuremath{\textnormal{adj}\left(#1\right)}} % adjoint
\newcommand{\obs}[1]{\textcolor{red}{(#1)}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}

% |a| -> absolute value of a, which is a scalar
\newcommand\abs[1]{\left\lvert#1\right\rvert}

% Set page size and margins
% Replace `letterpaper' with`a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

% title packages
\usepackage{authblk}

% bmatrix with additional gaps
\usepackage{tabstackengine}
\stackMath
\setstackgap{L}{30pt} % vertical gap
\setstacktabbedgap{10pt} % horizontal gap
\def\lrgap{\kern6pt}
\def\xbracketVectorstack#1{\left[\lrgap\Vectorstack{#1}\lrgap\right]} % use it for vectors
\def\xbracketMatrixstack#1{\left[\lrgap\tabbedCenterstack{#1}\lrgap\right]} % use it for matrices

% biblatex
\usepackage[backend=bibtex, sorting=none, style=numeric-comp, defernumbers=true]{biblatex} % using biblatex
\addbibresource{refs.bib} % add reference file
% for each cited reference create a category named "cited"

% strike out texts
\usepackage{soul}

% begin
\title{\textbf{The Guide for Matrix Calculus}  \vspace{-.3cm}}
\author{Rubem Vasconcelos Pacelli\\
  {\tt rubem.engenharia@gmail.com}}
\affil{Department of Teleinformatics Engineering, Federal University of Ceará.\\Fortaleza, Ceará, Brazil. \vspace{-.5cm}}

\begin{document}
\maketitle
\tableofcontents
\newpage
\section{Introduction}
Since my Master's degree, I've been struggling with matrix differentiation as I could not find good references that cover it nicely. The bibliographies I found at that time were books from Economics \cite{dhrymes1978mathematics}, but they use an \st{weird} unfamiliar notation.

After delving a lot, I finally found a good reference from Professor Randal's class note \cite{barnes2006matrix}. However, to my surprise, when I tried to apply those matrix differentiation propositions, I got ``wrong'' answers! The truth is that \emph{Matrix Calculus notation is severely fragmented and there is no consensus among the researchers over which notation to follow}. Fortunately, there are two major ways to represent a derivative of a vector \cite{Singh}. If you do not select the author's representation, you will end up with the transposed result. The first representation is called Jacobian formulation or numerator layout, while the second one is called Hessian formulation or denominator layout. Nevertheless, even for the same layout, some conventions need to be stated.

Due to the lack of references and the need to have one, I decided to make this quick guide. The goal here is twofold: make an introduction to both representations and derive the partial derivatives for the most common Matrix Calculus expressions you came across. Most of the problems will be defined in real space, but some will be extended to the complex plane. In the end, I hope to have a consistent reference guide to understand an author's book that uses a different notation. The way I define in Section \ref{sec:denominator} will be the way I adopt in my papers and in this guide, but I will let you know when some point is not consensus among the authors. Furthermore, I will adopt only apply the Hessian (or demodulator) formulation since it matches with the notation commonly used by authors of the field of Signal Processing, Machine Learning, and Optimization Theory.

\subsection{Curated reference list}
The following list shows some references you can rely on besides this guide when it comes to Matrix Calculus (in decreasing order of importance):
\begin{itemize}
    \item Searle, Shayle R., and Andre I. Khuri. Matrix algebra useful for statistics \cite{searle2017matrix}: A classical book with the first edition in 1987. It treats Matrix Calculus in chapter 9 of the second edition.
    \item Hj\o rungnes, Are, and David Gesbert. Complex-valued matrix differentiation: Techniques and key results \cite{hjorungnes2011complex}. An advanced book that uses Wirtinger calculus to differentiate complex-valued matrices. You might prefer his article \cite{hjorungnes2007complex} which summarizes the key results.
    \item Matrix Calculus You Need For Deep Learning \cite{parrMatrixCalculusYou2018}: Preprint article focused on its application on Machine Learning. It offers the best tradeoff between comprehension and brevity as it is just 33 pages long.
    \item You can find good references in these book appendices: Dattoro \cite{dattorroConvexOptimizationEuclidean2010}, appendix D; Bishop \cite{bishopPatternRecognitionMachine2006}, appendix C; Simon Haykin \cite{haykin2009neural}, appendix B (Wirtinger calculus).
    \item Old and New Matrix Algebra Useful for Statistics, Thomas Minka \cite{ThomasMinka}: Another good guide for matrix algebra hosted on the personal site of the author. It has very well-referenced and focuses more on the concepts of Matrix Calculus than solving the derivatives.
    \item The Matrix Cookbook \cite{petersen2008matrix}: Good reference to see Matrix Calculus results rather than understanding how they were solved.
    \item Lecture notes in Introduction to Machine Learning, from Carnegie Mellon University \cite{Singh}: A short lecture but with a clear explanation of the difference between numerator and denominator layout.
    \item Wikipedia \cite{Matrixca44:online}: Maybe it is not the most reliable source, but it is very comprehensive.
    \item Professor Randal's class note \cite{barnes2006matrix}: It has the same purpose as this guide, that is, to solve common matrix differentiation, but it is done using the numerator layout. Such a layout is rarely used for references herein cited.
\end{itemize}

Differentiation solutions that were collected from other sources will be referenced, while solution that I derived by myself will not have any reference. Obviously, this guide may have errors (I hope not). If you find it, feel free to reach out through email or simply make a pull request on my \href{https://github.com/tapyu/tapyu.github.io/tree/master/notes/posts/matrix_diff_ref/latex}{Github}.

\section{Notation and nomeclatures}
\label{sec:notation}

Let
\begin{align}
    \mathbf{A} = \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \dots & a_{mn} \\
    \end{bmatrix} \in \mathbb{C}^{m \times n}
\end{align}
be a complex matrix with dimension equal to \(m \times n\), where \(a_{ij} \in \mathbb{C}\) is its element in the position \((i,j)\). Similarly, a complex vector is defined by
\begin{align}
    \mathbf{x} = \begin{bmatrix}
        x_1 \\
        x_2 \\
        \vdots \\
        x_n
    \end{bmatrix}  \in \mathbb{C}^{n},
\end{align}
which may also be denoted as an \(n\)-tuple, \((x_1, x_2, \dots, x_n)\), when more convenient.

For scalars, italic Roman (\(a,b,c,\dots\)) represents constants (known values), while italic Greek\linebreak (\(\alpha, \beta, \gamma, \dots\)) represents variables (unknown values). For vectors, Roman bold lowercase letters will be adopted, where the initial letters (\(\mathbf{a}, \mathbf{b}, \mathbf{c}, \dots\)) represent constants (known values) and the final letters (\(\mathbf{x}, \mathbf{y}, \mathbf{w}, \dots\)) represent variables (unknown values). The same thing goes for matrices, but it is denoted as uppercase letters instead of lowercase. The letter \(\mathbf{z}\) (or \(\mathbf{Z}\), for matrices) will only be used when the vector (or matrix) is complex-valued. In any other case, the vector or matrix will be real-valued. Finally, the operators \(\cdot^{\trans}\), \(\cdot^{\hermit}\), \(\cdot^*\) \(\text{tr}(\cdot)\), \(\textnormal{adj}(\cdot)\), and \(\abs{\cdot}\) denote, respectively, the transpose, the hermitian, the conjugate, the trace, the adjoint, and the determinant (or absolute value when the operand is a scalar).

\subsection{Jacobian formulation (numerator layout)}

In the Jacobian formulation (also called numerator layout), the derivative matrix is written laying out the numerator in its shape, while the denominator has its shape transposed (you will understand it better as soon as you see the definitions).

\subsubsection{Vector-vector, scalar-vector, and vector-scalar derivatives}
Consider two vectors \(\mathbf{x} \in \mathbb{R}^n\) and \(\mathbf{y} \in \mathbb{R}^m\). The partial derivative of each element in \(\mathbf{y}\) by each element in \(\mathbf{x}\) is represented as
\begin{align}
    \label{eq:jacobian-formulation}
    \left[ \dfrac{\partial \mathbf{y}}{\partial \mathbf{x}} \right]_{\textnormal{Num}} = \renewcommand{\arraystretch}{2.6} \begin{bmatrix}
        \dfrac{\partial y_1}{\partial \mathbf{x}} \\
        \dfrac{\partial y_2}{\partial \mathbf{x}} \\ 
        \vdots \\ 
        \dfrac{\partial y_m}{\partial \mathbf{x}}
    \end{bmatrix} = \renewcommand{\arraystretch}{1.8}
    \begin{bmatrix}
        \dfrac{\partial y_1}{\partial x_1} & \dfrac{\partial y_1}{\partial x_2} & \dots & \dfrac{\partial y_1}{\partial x_n} \\
        \dfrac{\partial y_2}{\partial x_1} & \dfrac{\partial y_2}{\partial x_2} & \dots & \dfrac{\partial y_2}{\partial x_n} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial y_m}{\partial x_1} & \dfrac{\partial y_m}{\partial x_2} & \dots & \dfrac{\partial y_m}{\partial x_n} \\
    \end{bmatrix} \in \mathbb{R}^{m\times n}.
\end{align}

We can infer what is the shape of \(\dfrac{\partial y}{\partial \mathbf{x}}\) and \(\dfrac{\partial \mathbf{y}}{\partial x}\) by changing the respective vector sizes in \eqref{eq:jacobian-formulation}.

\subsubsection{Matrix-scalar derivative (tangent matrix)}
The partial derivative \(\dfrac{\partial \mathbf{Y}}{\partial x}\) (usually called tangent matrix) is defined for the numerator layout as
\begin{align}
    \label{eq:tangent-num}
    \left[\dfrac{\partial \mathbf{Y}}{\partial x}\right]_{\textnormal{Num}} \renewcommand{\arraystretch}{1.8} = \begin{bmatrix}
        \dfrac{\partial y_{11}}{\partial x} & \dfrac{\partial y_{12}}{\partial x} & \dots & \dfrac{\partial y_{1n}}{\partial x} \\
        \dfrac{\partial y_{21}}{\partial x} & \dfrac{\partial y_{22}}{\partial x} & \dots & \dfrac{\partial y_{2n}}{\partial x} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial y_{m1}}{\partial x} & \dfrac{\partial y_{m2}}{\partial x} & \dots & \dfrac{\partial y_{mn}}{\partial x} \\
    \end{bmatrix} \in \mathbb{R}^{m \times n},
\end{align}
where \(\mathbf{Y} \in \mathbb{R}^{m \times n}\).

\subsubsection{Scalar-matrix derivative (gradient matrix)}
The partial derivative of \(\dfrac{\partial y}{\partial \mathbf{X}}\) (usually called gradient matrix) is given by
\begin{align}
    \label{eq:gradient-matrix-num}
    \renewcommand{\arraystretch}{1.8}
			\left[\dfrac{\partial y}{\partial \mathbf{X}}\right]_{\textnormal{Num}} = \begin{bmatrix}
				\dfrac{\partial y}{\partial x_{11}} & \dfrac{\partial y}{\partial x_{21}} & \dots & \dfrac{\partial y}{\partial x_{m1}} \\
				\dfrac{\partial y}{\partial x_{12}} & \dfrac{\partial y}{\partial x_{22}} & \dots & \dfrac{\partial y}{\partial x_{m2}} \\
				\vdots & \vdots & \ddots & \vdots \\
				\dfrac{\partial y}{\partial x_{1n}} & \dfrac{\partial y}{\partial x_{2n}} & \dots & \dfrac{\partial y}{\partial x_{mn}} \\
			\end{bmatrix} \in \mathbb{R}^{n \times m},
\end{align}
where \(\mathbf{X} \in \mathbb{R}^{m \times n}\).

\subsubsection{Row vector-scalar and scalar-row vector derivatives}

From these definitions, we can infer two nonobvious equalities that are rather useful when handling matrix differentiations. If we consider a special case of the gradient matrix \eqref{eq:gradient-matrix-num}, when $m=1 \therefore \mathbf{X} = \mathbf{x}^\top \in \mathbb{R}^{1\times n}$, where $\mathbf{x} \in \mathbb{R}^n$, we have that
\begin{align}
    \left[\dfrac{\partial \alpha}{\partial \mathbf{x}^\top}\right]_{\textnormal{Num}} = \renewcommand{\arraystretch}{2.6}\begin{bmatrix}
        \dfrac{\partial \alpha}{\partial x_{1}} \\
        \dfrac{\partial \alpha}{\partial x_{2}} \\
        \vdots \\
        \dfrac{\partial \alpha}{\partial x_{n}}\vspace{1.3ex}
    \end{bmatrix} \in \mathbb{R}^{n}.
\end{align}
However, by comparing with \eqref{eq:jacobian-formulation}, one can state that
\begin{align}
    \left[\dfrac{\partial \alpha}{\partial \mathbf{x}}\right]_{\textnormal{Num}} = \begin{bmatrix} \dfrac{\partial \alpha}{\partial x_{1}} & \dfrac{\partial \alpha}{\partial x_{2}} & \cdots & \dfrac{\partial \alpha}{\partial x_{n}} \end{bmatrix} \in \mathbb{R}^{1 \times n},
\end{align}
Therefore,
\begin{align}
    \label{eq:hidden-num}
    \left[\dfrac{\partial \alpha}{\partial \mathbf{x}^\top}\right]_{\textnormal{Num}} = \left[\dfrac{\partial \alpha}{\partial \mathbf{x}}\right]_{\textnormal{Num}}^\top
\end{align}

Similarly, from \eqref{eq:tangent-num}, when $m=1 \therefore \mathbf{Y} = \mathbf{y}^\top \in \mathbb{R}^{1\times n}$, where $\mathbf{y} \in \mathbb{R}^n$, we have that

\begin{align}
    \left[\dfrac{\partial \mathbf{y}^\top}{\partial \alpha}\right]_{\textnormal{Num}} = \renewcommand{\arraystretch}{2.6}\begin{bmatrix}
        \dfrac{\partial y_{1}}{\partial \alpha} &
        \dfrac{\partial y_{2}}{\partial \alpha} &
        \cdots &
        \dfrac{\partial y_{n}}{\partial \alpha}\vspace{1.3ex}
    \end{bmatrix} \in \mathbb{R}^{1 \times n}.
\end{align}

However, from \eqref{eq:jacobian-formulation}, we also have that

\begin{align}
    \left[\dfrac{\partial \mathbf{y}}{\partial \alpha}\right]_{\textnormal{Num}} = \renewcommand{\arraystretch}{2.6}\begin{bmatrix}
        \dfrac{\partial y_{1}}{\partial \alpha} \\
        \dfrac{\partial y_{2}}{\partial \alpha} \\
        \cdots \\
        \dfrac{\partial y_{n}}{\partial \alpha}\vspace{1.3ex}
    \end{bmatrix} \in \mathbb{R}^{n}.
\end{align}

Therefore,
\begin{align}
    \left[\dfrac{\partial \mathbf{y}^\top}{\partial \alpha}\right]_{\textnormal{Num}} = \left[\dfrac{\partial \mathbf{y}}{\partial \alpha}\right]_{\textnormal{Num}}^\top.
\end{align}

\subsubsection{Jacobian matrix for the numerator layout}
In the numerator layout, the notation for the Jacobian matrix is given by
\begin{align}
    \label{eq:jacobian-numerator}
    \mathbf{J} = \left[ \frac{\partial f(\mathbf{x})}{\partial \mathbf{x}} \right]_{\textnormal{Num}}.
\end{align}

where \(f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}\) is a vector function. By calling \eqref{eq:jacobian-formulation}, it is clear that the Jacobian takes the form
\begin{align}
    \label{eq:jacobian-matrix}
    \mathbf{J} = \renewcommand{\arraystretch}{2.6} \begin{bmatrix}
        \dfrac{\partial f_1}{\partial \mathbf{x}} \\
        \dfrac{\partial f_2}{\partial \mathbf{x}} \\ 
        \vdots \\ 
        \dfrac{\partial f_m}{\partial \mathbf{x}}
    \end{bmatrix} = \begin{bmatrix}
        \dfrac{\partial f_1}{\partial x_1} & \dfrac{\partial f_1}{\partial x_2} & \dots & \dfrac{\partial f_1}{\partial x_n} \\
        \dfrac{\partial f_2}{\partial x_1} & \dfrac{\partial f_2}{\partial x_2} & \dots & \dfrac{\partial f_2}{\partial x_n} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial f_m}{\partial x_1} & \dfrac{\partial f_m}{\partial x_2} & \dots & \dfrac{\partial f_m}{\partial x_n} \\
    \end{bmatrix} \in \mathbb{R}^{m \times n},
\end{align}
where \(f = (f_1, f_2, \dots, f_m)\), being \(f_i: \mathbb{R}^n \rightarrow \mathbb{R}\), for \(1 \leq i \leq m\). The numerator layout is also called the Jacobian formulation due to the fact that it is represented without the need for the transpose operator.

\subsubsection{Hessian matrix for the numerator layout}

The Matrix Calculus notation for the Hessian matrix in the numerator layout is given by
\begin{align}
    \label{eq:hessian-eq-numerator}
    \mathbf{H} = \dfrac{\partial^{2} f(\mathbf{x})}{\partial \mathbf{x} \partial \mathbf{x}^\top} = \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        \dfrac{\partial^{2} f}{\partial x_1^2} & \dfrac{\partial^{2} f}{\partial x_1 \partial x_2} & \cdots & \dfrac{\partial^{2} f}{\partial x_1 \partial x_n} \\
        \dfrac{\partial^{2} f}{\partial x_2 \partial x_1} & \dfrac{\partial^{2} f}{\partial x_2^2} & \cdots & \dfrac{\partial^{2} f}{\partial x_2 \partial x_n} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial^{2} f}{\partial x_n \partial x_1} & \dfrac{\partial^{2} f}{\partial x_n \partial x_2} & \dots & \dfrac{\partial^{2} f}{\partial x_n^2}
    \end{bmatrix} \in \mathbb{R}^{n \times n},
\end{align}
where \(f: \mathbb{R}^{n} \rightarrow \mathbb{R}\) and \(\mathbf{x} \in \mathbb{R}^{n}\). We can rewrite it by recalling \eqref{eq:hidden-num}, that is,
\begin{align}
    \mathbf{H} = \dfrac{\partial^{2} f(\mathbf{x})}{\partial \mathbf{x} \partial \mathbf{x}^\top} = \dfrac{\partial }{\partial \mathbf{x}}\left( \dfrac{\partial f(\mathbf{x})}{\partial \mathbf{x}^\top} \right) = \dfrac{\partial }{\partial \mathbf{x}}\left( \dfrac{\partial f(\mathbf{x})}{\partial \mathbf{x}} \right)^\top.
\end{align}
Note that \(\left( \dfrac{\partial f(\mathbf{x})}{\partial \mathbf{x}} \right)^\top = \left( \dfrac{\partial f(\mathbf{x})}{\partial x_1}, \dfrac{\partial f(\mathbf{x})}{\partial x_2}, \dots, \dfrac{\partial f(\mathbf{x})}{\partial x_n} \right)\) is a vector. Therefore, we have a vector-vector derivate and, by applying the differentiation as shown in \eqref{eq:jacobian-formulation}, we get \(\mathbf{H}\).

\subsection{Hessian formulation (denominator layout)}
\label{sec:denominator}

In the Hessian formulation (also called denominator layout), the derivative matrix is written laying out the denominator in its shape, while the numerator has its shape transposed.

\subsubsection{Vector-vector, scalar-vector, and vector-scalar derivatives}
Consider two vectors \(\mathbf{x} \in \mathbb{R}^n\) and \(\mathbf{y} \in \mathbb{R}^m\). The partial derivative of each element in \(\mathbf{y}\) by each element in \(\mathbf{x}\) is represented as

\begin{align}
    \label{eq:denominator-layout}
    \left[ \dfrac{\partial \mathbf{y}}{\partial \mathbf{x}} \right]_{\textnormal{Den}} = \begin{bmatrix}
        \dfrac{\partial y_1}{\partial \mathbf{x}} & \dfrac{\partial y_2}{\partial \mathbf{x}} & \cdots & \dfrac{\partial y_3}{\partial \mathbf{x}}
    \end{bmatrix} = \begin{bmatrix}
        \dfrac{\partial y_1}{\partial x_1} & \dfrac{\partial y_2}{\partial x_1} & \dots & \dfrac{\partial y_m}{\partial x_1} \\
        \dfrac{\partial y_1}{\partial x_2} & \dfrac{\partial y_2}{\partial x_2} & \dots & \dfrac{\partial y_m}{\partial x_2} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial y_1}{\partial x_n} & \dfrac{\partial y_2}{\partial x_n} & \dots & \dfrac{\partial y_m}{\partial x_n} \\
    \end{bmatrix} \in \mathbb{R}^{n\times m}.
\end{align}
We can infer what is the shape of \(\dfrac{\partial y}{\partial \mathbf{x}}\) and \(\dfrac{\partial \mathbf{y}}{\partial x}\) by changing the respective vector sizes in \eqref{eq:denominator-layout}.

\subsubsection{Matrix-scalar derivative (tangent matrix)}
The partial derivative \(\dfrac{\partial \mathbf{Y}}{\partial x}\) (usually called tangent matrix) is defined for the denominator layout as
\begin{align}
    \label{eq:tangent-matrix-den}
    \left[\dfrac{\partial \mathbf{Y}}{\partial x}\right]_{\textnormal{Den}} \renewcommand{\arraystretch}{1.8} = \begin{bmatrix}
        \dfrac{\partial y_{11}}{\partial x} & \dfrac{\partial y_{21}}{\partial x} & \dots & \dfrac{\partial y_{m1}}{\partial x} \\
        \dfrac{\partial y_{12}}{\partial x} & \dfrac{\partial y_{22}}{\partial x} & \dots & \dfrac{\partial y_{m2}}{\partial x} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial y_{1n}}{\partial x} & \dfrac{\partial y_{2n}}{\partial x} & \dots & \dfrac{\partial y_{mn}}{\partial x} \\
    \end{bmatrix} \in \mathbb{R}^{n \times m},
\end{align}
where \(\mathbf{Y} \in \mathbb{R}^{m \times n}\).% However, be aware that some authors do not follow this convention for the tangent matrix for the denominator layout \cite{Matrixca44:online}. For the sake of consistency (laying out the denominator and the transpose of the numerator), I will follow the convention as denoted in \eqref{eq:tangent-matrix-den}.

\subsubsection{Scalar-matrix derivative (gradient matrix)}
The partial derivative of \(\dfrac{\partial y}{\partial \mathbf{X}}\) (usually called gradient matrix) is given by
\begin{align}
    \label{eq:gradient-matrix-den}
    \renewcommand{\arraystretch}{1.8}
			\left[\dfrac{\partial y}{\partial \mathbf{X}}\right]_{\textnormal{Den}} = \begin{bmatrix}
				\dfrac{\partial y}{\partial x_{11}} & \dfrac{\partial y}{\partial x_{12}} & \dots & \dfrac{\partial y}{\partial x_{1n}} \\
				\dfrac{\partial y}{\partial x_{21}} & \dfrac{\partial y}{\partial x_{22}} & \dots & \dfrac{\partial y}{\partial x_{2n}} \\
				\vdots & \vdots & \ddots & \vdots \\
				\dfrac{\partial y}{\partial x_{m1}} & \dfrac{\partial y}{\partial x_{m2}} & \dots & \dfrac{\partial y}{\partial x_{mn}} \\
			\end{bmatrix} \in \mathbb{R}^{m \times n},
\end{align}
where \(\mathbf{X} \in \mathbb{R}^{m \times n}\).

\subsubsection{Row vector-scalar and scalar-row vector derivatives}

From these definitions, we can infer two nonobvious equalities that are rather useful when handling matrix differentiations. If we consider a special case of the gradient matrix, \eqref{eq:gradient-matrix-den}, when $m=1 \therefore \mathbf{X} = \mathbf{x}^\top \in \mathbb{R}^{1\times n}$, where $\mathbf{x} \in \mathbb{R}^n$, we have that
\begin{align}
    \left[\dfrac{\partial \alpha}{\partial \mathbf{x}^\top}\right]_{\textnormal{Den}} = \begin{bmatrix} \dfrac{\partial \alpha}{\partial x_{1}} &
        \dfrac{\partial \alpha}{\partial x_{2}} &
        \cdots &
        \dfrac{\partial \alpha}{\partial x_{n}} \end{bmatrix} \in \mathbb{R}^{1\times n},
\end{align}
however, by using we definition from \eqref{eq:denominator-layout}, it is also true to state that
\begin{align}
    \left[\dfrac{\partial \alpha}{\partial \mathbf{x}}\right]_{\textnormal{Den}} = \begin{bmatrix} \dfrac{\partial \alpha}{\partial x_{1}} \\
        \dfrac{\partial \alpha}{\partial x_{2}} \\
        \vdots \\
        \dfrac{\partial \alpha}{\partial x_{n}} \end{bmatrix} \in \mathbb{R}^{n}.
\end{align}

Therefore,
\begin{align}
    \label{eq:hidden-den}
    \left[\dfrac{\partial \alpha}{\partial \mathbf{x}^\top}\right]_{\textnormal{Den}} = \left[\dfrac{\partial \alpha}{\partial \mathbf{x}}\right]_{\textnormal{Den}}^\top
\end{align}

Similarly, from \eqref{eq:tangent-matrix-den}, when $m=1 \therefore \mathbf{Y} = \mathbf{y}^\top \in \mathbb{R}^{1\times n}$, where $\mathbf{y} \in \mathbb{R}^n$, we have that

\begin{align}
    \left[\dfrac{\partial \mathbf{y}^\top}{\partial \alpha}\right]_{\textnormal{Den}} = \renewcommand{\arraystretch}{2.6}\begin{bmatrix}
        \dfrac{\partial y_{1}}{\partial \alpha} \\
        \dfrac{\partial y_{2}}{\partial \alpha} \\
        \cdots \\
        \dfrac{\partial y_{n}}{\partial \alpha}\vspace{1.3ex}
    \end{bmatrix} \in \mathbb{R}^{n}.
\end{align}

However, from \eqref{eq:denominator-layout}, we also have that

\begin{align}
    \left[\dfrac{\partial \mathbf{y}}{\partial \alpha}\right]_{\textnormal{Den}} = \renewcommand{\arraystretch}{2.6}\begin{bmatrix}
        \dfrac{\partial y_{1}}{\partial \alpha} &
        \dfrac{\partial y_{2}}{\partial \alpha} &
        \cdots &
        \dfrac{\partial y_{n}}{\partial \alpha}\vspace{1.3ex}
    \end{bmatrix} \in \mathbb{R}^{1\times n}.
\end{align}

Therefore,
\begin{align}
    \left[\dfrac{\partial \mathbf{y}^\top}{\partial \alpha}\right]_{\textnormal{Den}} = \left[\dfrac{\partial \mathbf{y}}{\partial \alpha}\right]_{\textnormal{Den}}^\top.
\end{align}

\subsubsection{The Jacobian matrix for the denominator layout}
The Jacobian matrix in denominator notation is given by
\begin{align}
    \mathbf{J} & = \left[ \frac{\partial f(\mathbf{x})}{\partial \mathbf{x}}^{\top} \right]_{\textnormal{Den}} \\
    & = \frac{\partial f(\mathbf{x})}{\partial \mathbf{x}}^\top\\
    & = \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        \dfrac{\partial f_1(\mathbf{x})}{\partial \mathbf{x}}^\top \\ \dfrac{\partial f_2(\mathbf{x})}{\partial \mathbf{x}}^\top \\ \vdots \\ \dfrac{\partial f_m(\mathbf{x})}{\partial \mathbf{x}}^\top
    \end{bmatrix} \\
    & = \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        \dfrac{\partial f_1}{\partial x_1} & \dfrac{\partial f_1}{\partial x_2} & \dots & \dfrac{\partial f_1}{\partial x_n} \\
        \dfrac{\partial f_2}{\partial x_1} & \dfrac{\partial f_2}{\partial x_2} & \dots & \dfrac{\partial f_2}{\partial x_n} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial f_m}{\partial x_1} & \dfrac{\partial f_m}{\partial x_2} & \dots & \dfrac{\partial f_m}{\partial x_n} \\
    \end{bmatrix} \in \mathbb{R}^{m \times n},
\end{align}
where \(f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}\) and \(f = (f_1, f_2, \dots, f_m)\), being \(f_i: \mathbb{R}\rightarrow\mathbb{R}\) for \(1 \leq i \leq m\).

\subsubsection{The Hessian matrix for the denominator layout}
The Hessian matrix in the denominator layout is given by
\begin{align}
    \label{eq:hessian-eq-denomintor}
    \mathbf{H} = \left[ \dfrac{\partial^{2} f(\mathbf{x})}{\partial \mathbf{x}^2} \right]_{\textnormal{Den}} = \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        \dfrac{\partial^{2} f}{\partial x_1^2} & \dfrac{\partial^{2} f}{\partial x_1 \partial x_2} & \cdots & \dfrac{\partial^{2} f}{\partial x_1 \partial x_n} \\
        \dfrac{\partial^{2} f}{\partial x_2 \partial x_1} & \dfrac{\partial^{2} f}{\partial x_2^2} & \cdots & \dfrac{\partial^{2} f}{\partial x_2 \partial x_n} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial^{2} f}{\partial x_n \partial x_1} & \dfrac{\partial^{2} f}{\partial x_n \partial x_2} & \dots & \dfrac{\partial^{2} f}{\partial x_n^2}
    \end{bmatrix} \in \mathbb{R}^{n \times n},
\end{align}
where \(f: \mathbb{R}^{n} \rightarrow \mathbb{R}\) and \(\mathbf{x} \in \mathbb{R}^{n}\). The element \(\partial\mathbf{x}^2\) seems to be merely a convention which means that \(\partial\mathbf{x}\) is repeated twice, in the very same way it is done with scalar differentiations. It can be scaled to \(n\) consecutive derivatives without making the notation cumbersome (points to the denominator layout team).

\subsection{Comparative between Jacobian and Hessian formulations}

As you could have noticed,
\begin{align}
    \left[ \dfrac{\partial \mathbf{y}}{\partial \mathbf{x}} \right]_{\textnormal{Num}} & = \left[ \dfrac{\partial \mathbf{y}}{\partial \mathbf{x}} \right]_{\textnormal{Den}}^{\trans}, \\
    \left[ \dfrac{\partial \mathbf{Y}}{\partial x}\right]_{\textnormal{Num}} & = \left[ \dfrac{\partial \mathbf{Y}}{\partial x}\right]_{\textnormal{Den}}^{\trans}, \\
    \left[\dfrac{\partial y}{\partial \mathbf{X}}\right]_{\textnormal{Num}} & = \left[\dfrac{\partial y}{\partial \mathbf{X}}\right]_{\textnormal{Den}}^{\trans}, \\
    \left[\dfrac{\partial \alpha}{\partial \mathbf{x} \partial\mathbf{x}^\top}\right]_{\textnormal{Num}} & = \left[\dfrac{\partial \alpha}{\partial \mathbf{x}^{2}}\right]_{\textnormal{Den}},
\end{align}
where that last equation can be inferred from the discussion about the Hessian matrix.

That is the difference when you try to differentiate without paying attention to which representation the author adopted. The good news is that, as long as you differentiate it correctly, you can switch between the Jacobian and Hessian formulations by simply transposing the final result. Fortunately, the denominator layout is the most adopted by authors from areas related to Signal Processing and Machine Learning. That is why we will focus on the denominator layout hereafter (the notation \(\left[\cdot\right]_{\textnormal{Den}}\) will be dropped out since we do not need it anymore). % \footnote{First, you should apply \(\left[\cdot\right]_{\textnormal{Den}} = \left[\cdot\right]_{\textnormal{Num}}^{\trans}\) on partial derivatives that you get in the solution. Then, you apply the transpose to the whole solution.}

As a rule of thumb, keep in mind that:
\begin{itemize}
    \item \(\dfrac{\partial \mathbf{y}}{\partial \mathbf{x}}\) will yield a matrix.
    \item \(\dfrac{\partial \mathbf{Y}}{\partial x}\) will yield a matrix.
    \item \(\dfrac{\partial x}{\partial \mathbf{X}}\) will yield a matrix.
    \item \(\dfrac{\partial y}{\partial \mathbf{x}}\) will yield a vector.
    \item \(\dfrac{\partial \mathbf{y}}{\partial x}\) will yield a \(1\times n\) matrix (``row vector'').
    \item \(\mathbf{J} = \dfrac{\partial f(\mathbf{x})}{\partial \mathbf{x}}^\top\).
    \item \(\mathbf{H} = \dfrac{\partial^{2} f(\mathbf{x})}{\partial \mathbf{x}^2}\).
    \item \(\dfrac{\partial\alpha}{\partial \mathbf{x}^\top} = \dfrac{\partial\alpha}{\partial \mathbf{x}}^\top\) for \(\alpha\in \mathbb{R}\).
    \item \(\dfrac{\partial \mathbf{y}^\top}{\partial \alpha} = \dfrac{\partial \mathbf{y}}{\partial \alpha}^\top\) for \(\alpha\in \mathbb{R}\).
\end{itemize}

\subsection{Notations not widely agreed upon}
\label{sub-sec:notations-not-agreed}
Expressions such as \(\dfrac{\partial \mathbf{y}}{\partial \mathbf{X}}, \dfrac{\partial \mathbf{Y}}{\partial \mathbf{X}}\), or \(\dfrac{\partial \mathbf{Y}}{\partial \mathbf{x}}\) have no agreement for both Jacobian and Hessian notations. It is possible, however, to define the matrix-matrix derivative for both representations. The problem is that some authors define it in the most intuitive manner: For \(\dfrac{\partial \mathbf{Y}}{\partial \mathbf{X}}\), element in the position \((i,j)\) is \(\partial y_{ij}/\partial x_{ij}\). However, as we saw, it is inconsistent for both formulations as the matrix for the Jacobian (Hessian) formulation must lay out its denominator (numerator) in its transposed shape. Therefore, for a consistent numerator layout, we would have
\begin{align}
    \left[\frac{\partial \mathbf{Y}}{\partial\mathbf{X}}\right]_{\textnormal{Num}} = \begin{bmatrix}
        \dfrac{\partial y_{1,1}}{\partial x_{1,1}} & \dfrac{\partial y_{1,2}}{\partial x_{2,1}} & \dots & \dfrac{\partial y_{1,m}}{\partial x_{m,1}} \\
        \dfrac{\partial y_{2,1}}{\partial x_{1,2}} & \dfrac{\partial y_{2,2}}{\partial x_{2,2}} & \dots & \dfrac{\partial y_{2,n}}{\partial x_{m,2}} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial y_{n,1}}{\partial x_{1,n}} & \dfrac{\partial y_{n,2}}{\partial x_{2,n}} & \dots & \dfrac{\partial y_{n,m}}{\partial x_{m,n}} \\
    \end{bmatrix} \in \mathbb{R}^{n\times m},
\end{align}
and for a consistent denominator layout, we would have

\begin{align}
    \left[\frac{\partial \mathbf{Y}}{\partial\mathbf{X}}\right]_{\textnormal{Den}} = \begin{bmatrix}
        \dfrac{\partial y_{1,1}}{\partial x_{1,1}} & \dfrac{\partial y_{2,1}}{\partial x_{1,2}} & \dots & \dfrac{\partial y_{n,1}}{\partial x_{1,n}} \\
        \dfrac{\partial y_{1,2}}{\partial x_{2,1}} & \dfrac{\partial y_{2,2}}{\partial x_{2,2}} & \dots & \dfrac{\partial y_{n,2}}{\partial x_{2,n}} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial y_{1,m}}{\partial x_{m,1}} & \dfrac{\partial y_{2,m}}{\partial x_{m,2}} & \dots & \dfrac{\partial y_{n,m}}{\partial x_{m,n}} \\
    \end{bmatrix} \in \mathbb{R}^{m \times n},
\end{align}
where \(\mathbf{X}\in \mathbb{R}^{m\times n}\) and \(\mathbf{Y} \in \mathbb{R}^{n\times m}\). For expressions \(\dfrac{\partial \mathbf{y}}{\partial \mathbf{X}}\) and \(\dfrac{\partial \mathbf{Y}}{\partial \mathbf{x}}\) can the derived by taking them as special cases of the matrix-matrix formulation. Notwithstanding, keep in mind that both equations are not standard and their usage must be acknowledged by the reader.

\subsection{On the ambiguity of the notation \(\nabla^2\)}

It is common for some authors, such as Simon Haykin \cite{haykin2009neural}, to denote the gradient vector and the Hessian matrix as
\begin{align}
    \mathbf{g} = \nabla f = \frac{\partial f}{\partial \mathbf{x}}
\end{align}
and
\begin{align}
    \mathbf{H} = \nabla^{2} f = \frac{\partial^2 f}{\partial \mathbf{x}^2},
\end{align}
respectively. It is important to point out, however, that the notation \(\nabla^{2}\) might be ambiguous since it is also used in Vector Calculus to denote the Laplacian operator.

Vector Calculus is another branch of Mathematics that deals with differentiations and integrals of vector fields in three-dimensional space. It is used in different areas of Physics and Engineering, such as electromagnetic fields, fluid flow, etc. In Vector Calculus, the gradient vector is commonly defined as
\begin{align}
    \mathbf{\nabla} = \begin{bmatrix}
        \dfrac{\partial}{\partial x_1} \\
        \dfrac{\partial}{\partial x_2} \\
        \vdots \\
        \dfrac{\partial}{\partial x_n} \\
    \end{bmatrix}
\end{align}

And the gradient vector is denoted in the very same way we denote in Matrix Calculus, i.e., \(\nabla f\), where \(f\) is a potential (or scalar-valued) function. On the other hand, the symbol \(\nabla^2 f\) is often used to denote the Laplacian operator, which is defined as
\begin{align}
    \nabla^2 f = \nabla \cdot \nabla f = \dfrac{\partial f}{\partial x_1} + \dfrac{\partial f}{\partial x_2} + \dots \dfrac{\partial f}{\partial x_n},
\end{align}
where \(\cdot\) is the dot or inner product. Since Vector Calculus is far enough from Matrix Calculus in many applications, the notation \(\nabla^2 f\) is used in both fields without problems. However, when it is not the case, the notation \(\nabla^2 f\) must be avoided to denote the Hessian matrix.

\section{Identities}
\label{sec:identities}

We need to be cautious when applying the matrix differentiation identities since the element orders matter. For instance, for scalar elements, the product rule may be written as either \((fg)' = f'g + g'f\) or \((fg)' = g f' + f g'\). In Matrix Calculus, we do not have such a privilege.

\subsection{Chain rule}
\subsubsection{Univariate functions}
For scalar elements, the chain rule is given by
\begin{align}
    \dfrac{\partial w}{\partial v} = \dfrac{\partial w}{\partial x} \dfrac{\partial x}{\partial y} \dfrac{\partial y}{\partial v}.
\end{align}
Similarly, in matrix notation, we have
\begin{align}
    \label{eq:chain-1inter}
    \dfrac{\partial \mathbf{w}}{\partial \mathbf{v}} = \dfrac{\partial \mathbf{y}}{\partial \mathbf{v}} \dfrac{\partial \mathbf{x}}{\partial \mathbf{y}} \dfrac{\partial \mathbf{w}}{\partial \mathbf{x}},
\end{align}
where \(\mathbf{x} \in \mathbb{R}^{n}, \mathbf{y} \in \mathbb{R}^{m}, \mathbf{v} \in \mathbb{R}^{p}\), and \(\mathbf{w} \in \mathbb{R}^q\). In this expression, \(\mathbf{w}\) depends on \(\mathbf{x}\), \(\mathbf{x}\) depends on \(\mathbf{y}\), and \(\mathbf{y}\) depends on \(\mathbf{z}\). The number of elements in the chain rule can be increased indiscriminately. The main point here is that \emph{the chain rule in Matrix Calculus notation must be placed backward when compared with the standard chain rule of scalar elements}.

\subsubsection{Multivariate functions}

In the previous section, we had a case where \(\mathbf{w}\) depends on \(\mathbf{x}\), which depends on \(\mathbf{y}\), which depends on \(\mathbf{v}\). If \(\mathbf{w} = f(\mathbf{x}), \mathbf{x} = g(\mathbf{y})\), and \(\mathbf{y} = h(\mathbf{v})\), then \(f, g\), and \(h\) are functions of one variable, also called univariate functions. However, we might find a situation where \(\mathbf{w} = f(\mathbf{x}, \mathbf{y})\) is a function of two (or more) variables.

For scalar elements, we can find partial derivatives of multivariate functions by considering that \(w = f(x, y)\) is differentiable on \(x\) and \(y\). The chain rule becomes
\begin{align}
    \dfrac{\partial w}{\partial z} = \dfrac{\partial w}{\partial x} \dfrac{\partial x}{\partial z} + \dfrac{\partial w}{\partial y} \dfrac{\partial y}{\partial z}.
\end{align}

Similarly, for Matrix Calculus notation, we have
\begin{align}
    \label{eq:chain-multi-inter}
    \dfrac{\partial \mathbf{w}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \dfrac{\partial \mathbf{w}}{\partial \mathbf{x}} + \dfrac{\partial \mathbf{y}}{\partial \mathbf{z}} \dfrac{\partial \mathbf{w}}{\partial \mathbf{y}}
\end{align}

Note the backward placement of each summation term. This expression can be used for an unrestricted number of variables.

\subsection{Sum (or minus) rule}
\subsubsection{Vector-vector derivative}
Let \(\mathbf{x}, \mathbf{y} \in \mathbb{R}^{m}\) and \(a, b \in \mathbb{R}\), where \(\mathbf{x}\) and \(\mathbf{y}\) depend on \(\mathbf{w} \in \mathbb{R}^{n}\), but \(a\) and \(b\) do not. Therefore,
\begin{align}
    \dfrac{\partial (a\mathbf{x} \pm b\mathbf{y})}{\partial\mathbf{w}} = a\dfrac{\partial \mathbf{x}}{\partial\mathbf{w}} \pm b\dfrac{\partial \mathbf{y}}{\partial\mathbf{w}}
\end{align}
\subsubsection{Matrix-scalar derivative}
Another is when you have
\begin{align}
    \dfrac{\partial \left( a\mathbf{X} \pm b\mathbf{Y} \right)}{\partial \alpha},
\end{align}
where \(\mathbf{X}, \mathbf{Y} \in \mathbb{R}^{m \times n}\) depend on \(\alpha \in \mathbb{R}\). The solution is
\begin{align}
    \dfrac{\partial (a\mathbf{X} \pm b\mathbf{Y})}{\partial\mathbf{\alpha}} = a\dfrac{\partial \mathbf{X}}{\partial \alpha} \pm b \dfrac{\partial \mathbf{Y}}{\partial \alpha}.
\end{align}
\subsubsection{Scalar-matrix derivative}
The scalar-matrix derivative has a similar result, i.e.,
\begin{align}
    \dfrac{\partial \left( ax \pm by \right)}{\partial \mathbf{W}} = a\dfrac{\partial x}{\partial \mathbf{W}} \pm b \dfrac{\partial y}{\partial \mathbf{W}},
\end{align}
where \(x, y \in \mathbb{R}\) depend on \(\mathbf{W} \in \mathbb{R}^{m\times n}\), but \(a,b \in \mathbb{R}\) do not.

\subsection{Product rule}
\subsubsection{Vector-vector derivative}
Let \(w \in \mathbb{R}\) and \(\mathbf{v} \in \mathbb{R}^{m}\), where both depend on \(\mathbf{x} \in \mathbb{R}^{n}\). Then,
\begin{align}
    \dfrac{\partial w \mathbf{v}}{\partial \mathbf{x}} = w \dfrac{\partial \mathbf{v}}{\partial \mathbf{x}} + \dfrac{\partial w}{\partial \mathbf{x}} \mathbf{v}^\trans.
\end{align}

Note that is not possible to apply the product rule when you have \(\mathbf{Wv}\), where \(\mathbf{W} \in \mathbb{R}^{n \times m}\) also depends on \(\mathbf{x}\). If you tried, you would get \(\partial\mathbf{W}/\partial\mathbf{x}\), which there is no consensus about this layout (vide Section \ref{sub-sec:notations-not-agreed}).
\subsubsection{Scalar-vector derivative}
Another possibility of applying the product rule is when you have \(\mathbf{w}^{\trans} \mathbf{v}\), where \(\mathbf{w} \in \mathbb{R}^{m}\) also depends on \(\mathbf{x} \in \mathbb{R}^{n}\). In this case, the dot product is given by
\begin{align}
    \label{eq:scalar-vector-product-rule}
    \dfrac{\partial \mathbf{w}^{\trans} \mathbf{v}}{\partial \mathbf{x}} = \dfrac{\partial \mathbf{v}}{\partial \mathbf{x}} \mathbf{w} + \dfrac{\partial \mathbf{w}}{\partial \mathbf{x}} \mathbf{v}.
\end{align}

\subsubsection{Scalar-matrix derivative}
It is still possible to apply the product rule to
\begin{align}
    \dfrac{\partial wv}{\partial \mathbf{X}},
\end{align}
where \(w,v \in \mathbb{R}\) depend on \(\mathbf{X} \in \mathbb{R}^{m \times n}\). In this case, we have
\begin{align}
    \dfrac{\partial wv}{\partial \mathbf{X}} = w \dfrac{\partial v}{\partial \mathbf{X}} + v \dfrac{\partial w}{\partial \mathbf{X}}.
\end{align}

\subsubsection{Matrix-scalar derivative}
The last case is when you have
\begin{align}
    \dfrac{\partial \mathbf{W}\mathbf{V}}{\partial \alpha},
\end{align}
where both \(\mathbf{W} \in \mathbb{R}^{m \times p}\) and \(\mathbf{V} \in \mathbb{R}^{p\times n}\) depend on \(\alpha \in \mathbb{R}\). In this case, we have
\begin{align}
    \label{eq:matrix-matrix-product-rule}
    \dfrac{\partial \mathbf{W}\mathbf{V}}{\partial \alpha} = \dfrac{\partial \mathbf{V}}{\partial \alpha}\mathbf{W}^{\trans} + \mathbf{V}^{T} \dfrac{\mathbf{W}}{\partial \alpha}
\end{align}


\section{Solution of Matrix Differentiations}\label{sec:diff}
We usually have two ways to solve matrix differentiation:
\begin{enumerate}
    \item Performing element-by-element operations in matrices and vectors;
    \item Preserving the Matrix Calculus notation, performing operations on the whole matrix/vector and, eventually, using some identities.
\end{enumerate}
The latter is usually more straightforward and less toilsome than the former and is therefore preferable.

The solutions in this Section will usually show the element-by-element solution and the solution by preserving the Matrix Calculus notation. For the element-by-element solutions, you only need to know that a scalar-vector derivate results in a vector for the denominator layout. All other shapes will naturally arise. For solutions with Matrix Calculus notation, you need to be acquainted with some of the identities shown in Section \ref{sec:identities}.

\subsection{\(\dfrac{\partial \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \mathbf{A}^\trans\)}

Let \(\mathbf{A}\in \mathbb{R}^{m\times n}\) and \(\mathbf{x} \in \mathbb{R}^{n}\), in which \(\mathbf{A}\) does not depend on \(\mathbf{x}\), we have that:
\begin{align}
    \dfrac{\partial \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = & \dfrac{\partial}{\partial \mathbf{x}} \left(
        \begin{bmatrix}
            a_{11} & a_{12} & \dots & a_{1n} \\
            a_{21} & a_{22} & \dots & a_{2n} \\
            \vdots & \vdots & \ddots & \vdots \\
            a_{m1} & a_{m2} & \dots & a_{mn} \\
        \end{bmatrix} \begin{bmatrix}
            x_{1} \\ x_{2} \\ \vdots \\ x_{n}
        \end{bmatrix} \right)  \\
    %
    = & \dfrac{\partial}{\partial \mathbf{x}} \left(\begin{bmatrix} 
        \sum_{j = 1}^n a_{1j}x_j \\
        \sum_{j = 1}^n a_{2j}x_j \\
        \vdots \\
        \sum_{j = 1}^n a_{mj}x_j
    \end{bmatrix}^\trans \right)  \\
    %
    = & \begin{bmatrix}
        \dfrac{\partial}{\partial \mathbf{x}}\left(\sum_{j = 1}^n {a_{1j}x_j}\right) & \dfrac{\partial}{\partial \mathbf{x}}\left(\sum_{j = 1}^n {a_{2j}x_j}\right) & \dots & \dfrac{\partial}{\partial \mathbf{x}}\left(\sum_{j = 1}^n {a_{mj}x_j}\right)
    \end{bmatrix}
\end{align}

Since a scalar-vector derivative is represented by a vector, we have that
\begin{align}
    %
    \dfrac{\partial \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = & \begin{bmatrix}
        \dfrac{\partial}{\partial x_1} \left( \sum_{j = 1}^n a_{1j}x_j \right) & 
        \dfrac{\partial}{\partial x_1} \left( \sum_{j = 1}^n a_{21j}x_j \right) & 
        \dots & 
        \dfrac{\partial}{\partial x_1} \left( \sum_{j = 1}^n a_{mj}x_j \right) \\
        \dfrac{\partial}{\partial x_2} \left( \sum_{j = 1}^n a_{1j}x_j \right) & 
        \dfrac{\partial}{\partial x_2} \left( \sum_{j = 1}^n a_{21j}x_j \right) & 
        \dots & 
        \dfrac{\partial}{\partial x_2} \left( \sum_{j = 1}^n a_{mj}x_j \right) \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial}{\partial x_n} \left( \sum_{j = 1}^n a_{1j}x_j \right) & 
        \dfrac{\partial}{\partial x_n} \left( \sum_{j = 1}^n a_{21j}x_j \right) & 
        \dots & 
        \dfrac{\partial}{\partial x_n} \left( \sum_{j = 1}^n a_{mj}x_j \right) \\
    \end{bmatrix}  \\
    %
    = & \begin{bmatrix}
        a_{11} & a_{21} & \dots & a_{n1} \\
        a_{12} & a_{22} & \dots & a_{n2} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{1m} & a_{2m} & \dots & a_{nm} \\
    \end{bmatrix}
\end{align}
\begin{align}
    \label{eq:lt-slution}
    \boxed{\dfrac{\partial \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \mathbf{A}^\trans \in \mathbb{R}^{n\times m}}
\end{align}

\subsection{\(\dfrac{\partial \mathbf{A}  \mathbf{x}}{\partial \mathbf{v}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{v}} \mathbf{A}^\trans\)}

Let \(\mathbf{x} \in \mathbb{R}^{n}\), \(\mathbf{v} \in \mathbb{R}^{p}\) and \(\mathbf{A} \in \mathbb{R}^{m\times n}\), where \(\mathbf{x}\) depends on \(\mathbf{v}\), but \(\mathbf{A}\) does not. Then
\begin{align}
    \dfrac{\partial \mathbf{A}  \mathbf{x}}{\partial \mathbf{v}} & =
    %
    \begin{bmatrix}
        \dfrac{\partial}{\partial \mathbf{v}}\sum_{i=1}^{n} a_{1i}x_i & \dfrac{\partial}{\partial \mathbf{v}}\sum_{i=1}^{n} a_{2i}x_i & \cdots & \dfrac{\partial}{\partial \mathbf{v}}\sum_{i=1}^{n} a_{mi}x_i
    \end{bmatrix} \\
    %
    & = \begin{bmatrix}
        \sum_{i=1}^{n} a_{1i}\dfrac{\partial x_i}{\partial \mathbf{v}} & \sum_{i=1}^{n} a_{2i}\dfrac{\partial x_i}{\partial \mathbf{v}} & \cdots & \sum_{i=1}^{n} a_{mi}\dfrac{\partial x_i}{\partial \mathbf{v}}
    \end{bmatrix} \\
    %
    & = \underbrace{\begin{bmatrix}
        \dfrac{\partial x_1}{\partial \mathbf{v}} & \dfrac{\partial x_2}{\partial \mathbf{v}} & \cdots & \dfrac{\partial x_n}{\partial \mathbf{v}}
    \end{bmatrix}}_{p \times n}
    %
    \underbrace{\begin{bmatrix}
        a_{11} & a_{21} & \dots & a_{m1} \\
        a_{12} & a_{22} & \dots & a_{m2} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{1n} & a_{2n} & \dots & a_{mn} \\
    \end{bmatrix}}_{n \times m}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{A}  \mathbf{x}}{\partial \mathbf{v}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{v}} \mathbf{A}^\trans \in \mathbb{R}^{p \times m}}
\end{align}

Observe that this result is equivalent to applying the chain rule (c.f. \eqref{eq:chain-1inter}), that is,
\begin{align}
    \dfrac{\partial \mathbf{A}  \mathbf{x}}{\partial \mathbf{v}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{v}} \dfrac{\partial \mathbf{A}  \mathbf{x}}{\partial \mathbf{x}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{v}} \mathbf{A}^\trans.
\end{align}

\subsection{\(\dfrac{\partial \mathbf{a}^\trans \mathbf{x}}{\partial \mathbf{x}} = \mathbf{a}\)}

Let \(\mathbf{a, x} \in \mathbb{R}^{n}\), in which \(\mathbf{a}\) does not depend on \(\mathbf{x}\). You can derive the derivative for the inner product by considering that \(\mathbf{a}^\trans\) is actually a \(1\times n\) matrix that transforms \(\mathbb{R}^{n}\) into \(\mathbb{R}\), and we already know what is the derivate of a \(\mathbf{Ax}\). Thus,
\begin{align}
    \dfrac{\partial \mathbf{a}^\trans \mathbf{x}}{\partial \mathbf{x}} = {\mathbf{a}^\trans}^\trans = \mathbf{a}.
\end{align}

Even though, if you want the step-by-step, here it is:
\begin{align}
    \dfrac{\partial \mathbf{a}^\trans \mathbf{x}}{\partial \mathbf{x}} &= \dfrac{\partial}{\partial \mathbf{x}} \left(
    \begin{bmatrix}
        a_1 & a_2 & \dots & a_n
    \end{bmatrix} \begin{bmatrix}
        x_{1} \\ x_{2} \\ \vdots \\ x_{n}
    \end{bmatrix} \right) \\
    & = \dfrac{\partial}{\partial \mathbf{x}} \left( \sum_{i = 1}^n a_ix_i \right) \\
    & = \begin{bmatrix}
        \dfrac{\partial}{\partial x_1} \left( \sum_{i = 1}^n a_ix_i \right) \\ \dfrac{\partial}{\partial x_2} \left( \sum_{i = 1}^n a_ix_i \right) \\ \vdots \\ \dfrac{\partial}{\partial x_n} \left( \sum_{i = 1}^n a_ix_i \right) 
    \end{bmatrix} = \begin{bmatrix}
        a_1 \\ a_2 \\ \vdots \\ a_n
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{a}^\trans \mathbf{x}}{\partial \mathbf{x}} = \mathbf{a} \in \mathbb{R}^{n}}
\end{align}

\subsection{\(\dfrac{\partial \mathbf{x}^\trans  \mathbf{a}}{\partial \mathbf{x}} = \mathbf{a}\)}

This one can be solved quickly by noticing that \(\mathbf{x}^\trans  \mathbf{a} = \mathbf{a}^\trans  \mathbf{x}\). Hence,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans  \mathbf{a}}{\partial \mathbf{x}} = \dfrac{\partial \mathbf{a}^\trans  \mathbf{x}}{\partial \mathbf{x}} = \mathbf{a}
\end{align}

Nevertheless, here is the step-by-step:
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{a}}{\partial \mathbf{x}} &= \dfrac{\partial}{\partial \mathbf{x}} \left(
    \begin{bmatrix}
        x_1 & x_2 & \dots & x_n
    \end{bmatrix} \begin{bmatrix}
        a_{1} \\ a_{2} \\ \vdots \\ a_{n}
    \end{bmatrix} \right) \\
    & = \dfrac{\partial}{\partial \mathbf{x}} \left( \sum_{i = 1}^n x_ia_i \right) \\
    & = \begin{bmatrix}
        \dfrac{\partial}{\partial x_1} \left( \sum_{i = 1}^n x_ia_i \right) \\ \dfrac{\partial}{\partial x_2} \left( \sum_{i = 1}^n x_ia_i \right) \\ \vdots \\ \dfrac{\partial}{\partial x_n} \left( \sum_{i = 1}^n x_ia_i \right) 
    \end{bmatrix} 
    = \begin{bmatrix}
        a_1 \\ a_2 \\ \vdots \\ a_n
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{a}}{\partial \mathbf{x}} = \mathbf{a} \in \mathbb{R}^{n}}
\end{align}

\subsection{\(\dfrac{\partial \mathbf{a}^\hermit  \mathbf{x}}{\partial \mathbf{x}} = \mathbf{a}^*\)}

Let \(\mathbf{a} \in \mathbb{C}^{n}\) and \(\mathbf{x}\in \mathbb{R}^{n}\), in which \(\mathbf{a}\) does not depend on \(\mathbf{x}\). Once again, we could say that
\begin{align}
    \dfrac{\partial \mathbf{a}^\hermit \mathbf{x}}{\partial \mathbf{x}} = {\mathbf{a}^\hermit}^\trans = \mathbf{a}^*
\end{align}

Nevertheless, here is the step-by-step:
\begin{align}
    \dfrac{\partial \mathbf{a}^\hermit \mathbf{x}}{\partial \mathbf{x}} &= \dfrac{\partial}{\partial \mathbf{x}} \left(
    \begin{bmatrix}
        a^*_1 & a^*_2 & \dots & a^*_n
    \end{bmatrix} \begin{bmatrix}
        x_{1} \\ x_{2} \\ \vdots \\ x_{n}
    \end{bmatrix} \right) 
    = \dfrac{\partial}{\partial \mathbf{x}} \left( \sum_{i = 1}^n a^*_ix_i \right) \\
\end{align}

Since a scalar-vector derivative is represented by a vector, we have that
\begin{align}
    \dfrac{\partial \mathbf{a}^\hermit \mathbf{x}}{\partial \mathbf{x}} &= \begin{bmatrix}
        \dfrac{\partial}{\partial x_1} \left( \sum_{i = 1}^n a^*_ix_i \right) \\ \dfrac{\partial}{\partial x_2} \left( \sum_{i = 1}^n a^*_ix_i \right) \\ \vdots \\ \dfrac{\partial}{\partial x_n} \left( \sum_{i = 1}^n a^*_ix_i \right) 
    \end{bmatrix}
    = \begin{bmatrix}
        a^*_1 \\ a^*_2 \\ \vdots \\ a^*_n
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{a}^\hermit \mathbf{x}}{\partial \mathbf{x}} = \mathbf{a}^* \in \mathbb{C}^{n}}
\end{align}

\subsection{\(\dfrac{\partial \mathbf{z}^\hermit \mathbf{a}}{\partial \mathbf{z}} = \mathbf{0}\)}
Let us define \(\mathbf{z}, \mathbf{a} \in \mathbb{C}^{n}\), where \(\mathbf{a}\) does not depend on \(\mathbf{z}\). Since \(\mathbf{z}^\hermit \mathbf{a} \neq \mathbf{a}^\hermit \mathbf{z}\), we have no choice but derive it:
\begin{align}
    \dfrac{\partial \mathbf{z}^\hermit \mathbf{a}}{\partial\mathbf{z}} & = \dfrac{\partial}{\partial\mathbf{z}} \left(
    \begin{bmatrix}
        z^*_1 & z^*_2 & \dots & z^*_n
    \end{bmatrix} \begin{bmatrix}
        a_{1} \\ a_{2} \\ \vdots \\ a_{n}
    \end{bmatrix} \right) \\
    & = \dfrac{\partial}{\partial\mathbf{z}} \left( \sum_{i = 1}^n z^*_ia_i \right) \\
    &= \begin{bmatrix}
            \dfrac{\partial}{\partial z_1} \left( \sum_{i = 1}^n z^*_ia_i \right) \\ \dfrac{\partial}{\partial z_2} \left( \sum_{i = 1}^n z^*_ia_i \right) \\ \vdots \\
            \dfrac{\partial}{\partial z_n} \left( \sum_{i = 1}^n z^*_ia_i \right)
        \end{bmatrix} \\
    & = \renewcommand{\arraystretch}{2.6}\begin{bmatrix}
        \dfrac{\partial z_1^*}{\partial z_1} \\ \dfrac{\partial z_2^*}{\partial z_2} \\ \vdots \\
        \dfrac{\partial z_n^*}{\partial z_n}
    \end{bmatrix}.
\end{align}
By recalling that \(\dfrac{\partial z^*}{\partial z} = 0\) \cite{hjorungnes2011complex}, we have that
\begin{align}
    \dfrac{\partial \mathbf{z}^\hermit \mathbf{a}}{\partial\mathbf{z}} = \begin{bmatrix}
        0 \\ 0 \\ \vdots \\ 0
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{z}^\hermit \mathbf{a}}{\partial\mathbf{z}} = \mathbf{0} \in \mathbb{C}^{n}}
\end{align}
where \(\mathbf{0}\) is the zero vector.

\subsection{\(\dfrac{\partial \mathbf{z}^\hermit \mathbf{a}}{\partial \mathbf{z}^*} = \mathbf{a}\)}
\obs{TODO} \cite{hjorungnes2011complex}

\subsection{\(\dfrac{\partial \mathbf{x}^\trans \mathbf{y}}{\partial \mathbf{v}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{v}}\mathbf{y} + \dfrac{\partial \mathbf{y}}{\partial \mathbf{v}}\mathbf{x}\)}
Let \(\mathbf{x},\mathbf{y} \in \mathbb{R}^{n}\) and \(\mathbf{v} \in \mathbb{R}^{m}\). Where \(\mathbf{x}\) and \(\mathbf{y}\) depend on \(\mathbf{v}\). Thus,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{y}}{\partial \mathbf{v}} & = \dfrac{\partial}{\partial \mathbf{v}}\sum_{i=1}^{n} y_ix_i \\
    & = \sum_{i=1}^{n} \dfrac{\partial y_ix_i}{\partial \mathbf{v}}.
\end{align}
Recalling that \((fg)' = f'g + g'f\), we have
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{y}}{\partial \mathbf{v}} & = \sum_{i=1}^{n} x_i\dfrac{\partial y_i}{\partial \mathbf{v}} + \sum_{i=1}^{n} y_i\dfrac{\partial x_i}{\partial \mathbf{v}} \\
    & = \begin{bmatrix}
        \sum_{i=1}^{n} x_i\dfrac{\partial y_i}{\partial v_1} \\
        \sum_{i=1}^{n} x_i\dfrac{\partial y_i}{\partial v_2} \\
        \vdots \\
        \sum_{i=1}^{n} x_i\dfrac{\partial y_i}{\partial v_m} \\
    \end{bmatrix} +
    \begin{bmatrix}
        \sum_{i=1}^{n} y_i\dfrac{\partial x_i}{\partial v_1} \\
        \sum_{i=1}^{n} y_i\dfrac{\partial x_i}{\partial v_2} \\
        \vdots \\
        \sum_{i=1}^{n} y_i\dfrac{\partial x_i}{\partial v_m}
    \end{bmatrix} \\
    %
    & = \begin{bmatrix}
        \dfrac{\partial y_1}{\partial v_1} & \dfrac{\partial y_2}{\partial v_1} & \cdots & \dfrac{\partial y_n}{\partial v_1} \\
        \dfrac{\partial y_1}{\partial v_2} & \dfrac{\partial y_2}{\partial v_2} & \cdots & \dfrac{\partial y_n}{\partial v_2} \\
        \vdots & \ddots & \vdots & \vdots \\
        \dfrac{\partial y_1}{\partial v_m} & \dfrac{\partial y_2}{\partial v_m} & \cdots & \dfrac{\partial y_n}{\partial v_m} \\
    \end{bmatrix} \mathbf{x} \\
    & \hspace{2.5ex} + \begin{bmatrix}
        \dfrac{\partial x_1}{\partial v_1} & \dfrac{\partial x_2}{\partial v_1} & \cdots & \dfrac{\partial x_n}{\partial v_1} \\
        \dfrac{\partial x_1}{\partial v_2} & \dfrac{\partial x_2}{\partial v_2} & \cdots & \dfrac{\partial x_n}{\partial v_2} \\
        \vdots & \ddots & \vdots & \vdots \\
        \dfrac{\partial x_1}{\partial v_m} & \dfrac{\partial x_2}{\partial v_m} & \cdots & \dfrac{\partial x_n}{\partial v_m} \\
    \end{bmatrix} \mathbf{y}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{y}}{\partial \mathbf{v}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{v}}\mathbf{y} + \dfrac{\partial \mathbf{y}}{\partial \mathbf{v}}\mathbf{x} \in \mathbb{R}^{m}}
\end{align}
Note that, if either \(\mathbf{x}\) or \(\mathbf{y}\) does not depend on \(\mathbf{v}\), just disregard \(\dfrac{\partial \mathbf{x}}{\partial \mathbf{v}}\mathbf{y}\) or \(\dfrac{\partial \mathbf{y}}{\partial \mathbf{v}}\mathbf{x}\), respectively. When none depends on \(\mathbf{v}\), the obvious result is the zero vector, \(\mathbf{0}\). A simpler way to solve it is to apply the scalar-vector product rule (see \eqref{eq:scalar-vector-product-rule}), that is,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{y}}{\partial \mathbf{v}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{v}} \dfrac{\partial \mathbf{x}^\trans \mathbf{y}}{\partial \mathbf{x}} + \dfrac{\partial \mathbf{y}}{\partial \mathbf{v}} \dfrac{\partial \mathbf{x}^\trans \mathbf{y}}{\partial \mathbf{y}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{v}}\mathbf{y} + \dfrac{\partial \mathbf{y}}{\partial \mathbf{v}}\mathbf{x}
\end{align}

\subsection{\(\dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{x}} = 2\mathbf{x}\)}
Let \(\mathbf{x} \in \mathbb{R}^{n}\). Thus,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{x}} & = \dfrac{\partial}{\partial \mathbf{x}}\sum_{i=1}^{n} x_i^2 \\
    & = \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial \mathbf{x}} \\
    %
    & = \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial x_1} \\
        \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial x_2} \\
        \vdots \\
        \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial x_m} \\
    \end{bmatrix} \\
    %
    & = \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        2x_1 \\
        2x_2 \\
        \vdots \\
        2x_n \\
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{x}} = 2\mathbf{x} \in \mathbb{R}^n}
\end{align}

Note that this perfectly matches with the derivate of a quadratic scalar value, i.e., \(\frac{\diff x^2}{\diff x} = 2x\).

\subsection{\(\dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{v}} = 2\dfrac{\partial \mathbf{x}}{\partial \mathbf{v}}\mathbf{x}\)}
Let \(\mathbf{x} \in \mathbb{R}^{n}\) and \(\mathbf{v}\in \mathbb{R}^m\), where \(\mathbf{x}\) depends on \(\mathbf{v}\). Thus,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{v}} & = \dfrac{\partial}{\partial \mathbf{v}}\sum_{i=1}^{n} x_i^2 \\
    & = \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial \mathbf{v}} \\
    %
    & = \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial v_1} \\
        \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial v_2} \\
        \vdots \\
        \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial v_m} \\
    \end{bmatrix} \\
    %
    & = \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        \sum_{i=1}^{n} 2x_i\dfrac{\partial x_i}{\partial v_1} \\
        \sum_{i=1}^{n} 2x_i\dfrac{\partial x_i}{\partial v_2} \\
        \vdots \\
        \sum_{i=1}^{n} 2x_i\dfrac{\partial x_i}{\partial v_m} \\
    \end{bmatrix} \\
    & = 2 \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        \dfrac{\partial x_1}{\partial \mathbf{v}} & \dfrac{\partial x_2}{\partial \mathbf{v}} & \cdots & \dfrac{\partial x_n}{\partial \mathbf{v}}
    \end{bmatrix} \mathbf{x}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{v}} = 2\dfrac{\partial \mathbf{x}}{\partial \mathbf{v}}\mathbf{x} \in \mathbb{R}^m}
\end{align}

Note that this solution could also be solved by the chain rule (c.f. \eqref{eq:chain-1inter}) as follows
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{v}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{v}} \dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{x}} = 2\dfrac{\partial \mathbf{x}}{\partial \mathbf{v}}\mathbf{x} \in \mathbb{R}^m}
\end{align}

\subsection{\(\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \left(\mathbf{A}^\trans + \mathbf{A}\right) \mathbf{x}\)}
Let \(\mathbf{A}\in \mathbb{R}^{n\times n}\) and \(\mathbf{x} \in \mathbb{R}^{n}\), in which \(\mathbf{A}\) does not depend on \(\mathbf{x}\). For the quadratic form, it follows that
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} &= \dfrac{\partial}{\partial \mathbf{x}} \left(
    \begin{bmatrix}
        x_{1} & x_{2} & \dots & x_{n}
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & \dots & a_{nn} \\
    \end{bmatrix} \begin{bmatrix}
        x_{1} \\ x_{2} \\ \vdots \\ x_{n}
    \end{bmatrix} \right) \\
    &= \dfrac{\partial}{\partial \mathbf{x}} \left(
			\begin{bmatrix}
				\displaystyle \sum_{i = 1}^{n} x_{i}a_{i1} & 
				\displaystyle \sum_{i = 1}^{n} x_{i}a_{i2} & 
				\dots & 
				\displaystyle \sum_{i = 1}^{n} x_{i}a_{in}
			\end{bmatrix} \begin{bmatrix}
				x_{1} \\ x_{2} \\ \vdots \\ x_{n}
			\end{bmatrix} \right) \\
            &= \dfrac{\partial}{\partial \mathbf{x}} \left(
				\sum_{i = 1}^{n}\sum_{j = 1}^{n} a_{ij} x_{i} x_{j}
			\right).
\end{align}

Note that the element inside the parentheses is a scalar and that a scalar-vector derivative results in a vector, that is,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} &= \begin{bmatrix}
        \displaystyle \dfrac{\partial}{\partial x_1} \left( \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i} a_{ij} x_{j} \right) \\ 
        \displaystyle  \dfrac{\partial}{\partial x_2} \left( \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i} a_{ij} x_{j} \right) \\ 
        \vdots \\ 
        \displaystyle \dfrac{\partial}{\partial x_n} \left( \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i} a_{ij} x_{j} \right) 
    \end{bmatrix} \\
    & = \begin{bmatrix}
        \displaystyle 2x_1a_{11} + \sum_{\substack{j = 1 \\ j \neq 1}}^{n} a_{1j} x_{j} + \sum_{\substack{i = 1 \\ i \neq 1}}^{n} a_{i1} x_{i} \\
        \displaystyle 2x_2a_{22} + \sum_{\substack{j = 1 \\ j \neq 2}}^{n} a_{2j} x_{j} + \sum_{\substack{i = 1 \\ i \neq 2}}^{n} a_{i2} x_{i} \\
        \vdots \\
        \displaystyle 2x_na_{nn} + \sum_{\substack{j = 1 \\ j \neq n}}^{n} a_{nj} x_{j} + \sum_{\substack{i = 1 \\ i \neq n}}^{n} a_{in} x_{i} 
    \end{bmatrix} \\
    &= \begin{bmatrix}
        \displaystyle \sum_{j = 1}^{n} a_{1j} x_{j} \\
        \displaystyle \sum_{j = 1}^{n} a_{2j} x_{j} \\
        \vdots \\
        \displaystyle \sum_{j = 1}^{n} a_{nj} x_{j} 
    \end{bmatrix} +
    \begin{bmatrix}
        \displaystyle \sum_{i = 1}^{n} a_{i1} x_{i} \\
        \displaystyle \sum_{i = 1}^{n} a_{i2} x_{i} \\
        \vdots \\
        \displaystyle \sum_{i = 1}^{n} a_{in} x_{i} 
    \end{bmatrix} \\
    & = \mathbf{A}^\trans \mathbf{x} + \mathbf{A} \mathbf{x}
\end{align}
\begin{align}
    \label{eq:quadratic-solution}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \left(\mathbf{A}^\trans + \mathbf{A}\right) \mathbf{x} \in \mathbb{R}^{n}}
\end{align}
For the special case where \(\mathbf{A}\) is symmetric, we obtain
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = 2\mathbf{A} \mathbf{x} \in \mathbb{R}^{n}}
\end{align}

\subsection{\(\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{v}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{v}}\left( \mathbf{A} + \mathbf{A}^\trans \right) \mathbf{x}\)}
Let \(\mathbf{v} \in \mathbb{R}^{m}, \mathbf{x} \in \mathbb{R}^{n}\) and \(\mathbf{A}\in \mathbb{R}^{n\times n}\), where \(\mathbf{x}\) depends on \(\mathbf{v}\), but \(\mathbf{A}\) does not. For the quadratic form, it follows that
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{v}} &= \dfrac{\partial}{\partial \mathbf{v}} \left(
    \begin{bmatrix}
        x_{1} & x_{2} & \dots & x_{n}
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & \dots & a_{nn} \\
    \end{bmatrix} \begin{bmatrix}
        x_{1} \\ x_{2} \\ \vdots \\ x_{n}
    \end{bmatrix} \right) \\
    &= \dfrac{\partial}{\partial \mathbf{v}} \left(
			\begin{bmatrix}
				\displaystyle \sum_{i = 1}^{n} x_{i}a_{i1} & 
				\displaystyle \sum_{i = 1}^{n} x_{i}a_{i2} & 
				\dots & 
				\displaystyle \sum_{i = 1}^{n} x_{i}a_{in}
			\end{bmatrix} \begin{bmatrix}
				x_{1} \\ x_{2} \\ \vdots \\ x_{n}
			\end{bmatrix} \right) \\
            &= \dfrac{\partial}{\partial \mathbf{v}} \left(
				\sum_{i = 1}^{n}\sum_{j = 1}^{n} a_{ij} x_{i} x_{j}
			\right) \\
    &= \begin{bmatrix}
        \displaystyle \dfrac{\partial}{\partial v_1} \left( \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i} a_{ij} x_{j} \right) \\ 
        \displaystyle  \dfrac{\partial}{\partial v_2} \left( \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i} a_{ij} x_{j} \right) \\ 
        \vdots \\ 
        \displaystyle \dfrac{\partial}{\partial v_n} \left( \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i} a_{ij} x_{j} \right) 
    \end{bmatrix} \\
    &= \begin{bmatrix}
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} a_{ij} \dfrac{\partial x_{i}x_{j}}{\partial v_1} \\ 
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} a_{ij} \dfrac{\partial x_{i}x_{j}}{\partial v_2} \\ 
        \vdots \\ 
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} a_{ij} \dfrac{\partial x_{i}x_{j}}{\partial v_n} 
    \end{bmatrix}
\end{align}

Recalling that \((fg)' = f'g + g'f\), we have that
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{v}} &= \begin{bmatrix}
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{j}a_{ij} \dfrac{\partial x_{i}}{\partial v_1} +
        \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i}a_{ij} \dfrac{\partial x_{j}}{\partial v_1} \\ 
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{j}a_{ij} \dfrac{\partial x_{i}}{\partial v_2} +
        \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i}a_{ij} \dfrac{\partial x_{j}}{\partial v_2} \\ 
        \vdots \\ 
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{j}a_{ij} \dfrac{\partial x_{i}}{\partial v_n} +
        \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i}a_{ij} \dfrac{\partial x_{j}}{\partial v_n} 
    \end{bmatrix} \\
    &= \begin{bmatrix}
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{j}a_{ij} \dfrac{\partial x_{i}}{\partial v_1} \\
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{j}a_{ij} \dfrac{\partial x_{i}}{\partial v_2} \\
        \vdots \\ 
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{j}a_{ij} \dfrac{\partial x_{i}}{\partial v_n}
    \end{bmatrix} +
    \begin{bmatrix}
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i}a_{ij} \dfrac{\partial x_{j}}{\partial v_1} \\ 
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i}a_{ij} \dfrac{\partial x_{j}}{\partial v_2} \\ 
        \vdots \\ 
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i}a_{ij} \dfrac{\partial x_{j}}{\partial v_n} 
    \end{bmatrix} \\
    &= \begin{bmatrix}
       \dfrac{\partial x_{1}}{\partial \mathbf{v}} & \dfrac{\partial x_{2}}{\partial \mathbf{v}} & \cdots  & \dfrac{\partial x_{n}}{\partial \mathbf{v}}
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \dots & a_{mn} \\
    \end{bmatrix}
    \begin{bmatrix}
        x_{1} \\ x_{2} \\ \vdots \\ x_{n}
    \end{bmatrix} + \nonumber \\
    & \hspace{2.5ex} \begin{bmatrix}
        \dfrac{\partial x_{1}}{\partial \mathbf{v}} & \dfrac{\partial x_{2}}{\partial \mathbf{v}} & \cdots  & \dfrac{\partial x_{n}}{\partial \mathbf{v}}
     \end{bmatrix}
     \begin{bmatrix}
         a_{11} & a_{21} & \dots & a_{m1} \\
         a_{12} & a_{22} & \dots & a_{m2} \\
         \vdots & \vdots & \ddots & \vdots \\
         a_{1n} & a_{2n} & \dots & a_{mn} \\
     \end{bmatrix}
     \begin{bmatrix}
         x_{1} \\ x_{2} \\ \vdots \\ x_{n}
     \end{bmatrix} \\
     & = \dfrac{\partial \mathbf{x}}{\partial \mathbf{v}}\mathbf{A} \mathbf{x} + \dfrac{\partial \mathbf{x}}{\partial \mathbf{v}}\mathbf{A}^\trans \mathbf{x}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{v}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{v}}\left( \mathbf{A} + \mathbf{A}^\trans \right) \mathbf{x} \in \mathbb{C}^{m}}
\end{align}

For the special case where \(\mathbf{A}\) is symmetric, we get
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{v}} = 2\dfrac{\partial \mathbf{x}}{\partial \mathbf{v}}\mathbf{A} \mathbf{x} \in \mathbb{C}^{m}}
\end{align}

Note that the solution is much easier if we maintain the Matrix Calculus notation and apply the chain rule, that is,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{v}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{v}} \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{v}} \left(\mathbf{A}^\trans + \mathbf{A}\right) \mathbf{x},
\end{align}
where the last equality comes from \eqref{eq:quadratic-solution}.

\subsection{\(\dfrac{\partial \mathbf{b}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \mathbf{A}^\trans \mathbf{b}\)} \label{sec:bt-A-x}
Let \(\mathbf{x} \in \mathbb{R}^{n}\), \(\mathbf{b} \in \mathbb{R}^{m}\) and \(\mathbf{A}\in \mathbb{R}^{m\times n}\), where neither \(\mathbf{b}\) nor \(\mathbf{A}\) depend on \(\mathbf{x}\). It follows that

\begin{align}
    \dfrac{\partial \mathbf{b}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} &= \dfrac{\partial}{\partial \mathbf{x}} \left(
    \begin{bmatrix}
        b_{1} & b_{2} & \dots & b_{m}
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \dots & a_{mn} \\
    \end{bmatrix} \begin{bmatrix}
        x_{1} \\ x_{2} \\ \vdots \\ x_{n}
    \end{bmatrix} \right) \\
    &= \dfrac{\partial}{\partial \mathbf{x}} \left(
			\begin{bmatrix}
				\displaystyle \sum_{i = 1}^{m} a_{i1}b_{i} & 
				\displaystyle \sum_{i = 1}^{m} a_{i2}b_{i} & 
				\dots & 
				\displaystyle \sum_{i = 1}^{m} a_{in}b_{i}
			\end{bmatrix} \begin{bmatrix}
				x_{1} \\ x_{2} \\ \vdots \\ x_{n}
			\end{bmatrix} \right) \\
    &= \dfrac{\partial}{\partial \mathbf{x}} \left(
        \sum_{i = 1}^{m}\sum_{j = 1}^{n} a_{ij} b_{i} x_{j}
    \right) \\
    &= \sum_{i = 1}^{m}\sum_{j = 1}^{n} a_{ij} b_{i} \dfrac{\partial x_{j}}{\partial \mathbf{x}} \\
    &= \begin{bmatrix}
        \sum_{i = 1}^{m} a_{i1} b_{i} \\
        \sum_{i = 1}^{m} a_{i2} b_{i} \\
        \vdots \\
        \sum_{i = 1}^{m} a_{in} b_{i} \\
    \end{bmatrix} \\
    &= \begin{bmatrix}
        a_{11} & a_{21} & \dots & a_{m1} \\
        a_{12} & a_{22} & \dots & a_{m2} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{1n} & a_{2n} & \dots & a_{mn} \\
    \end{bmatrix}
    \begin{bmatrix}
        b_1 \\
        b_2 \\
        \vdots \\
        b_m
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{b}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \mathbf{A}^\trans \mathbf{b} \in \mathbb{R}^{n}}
\end{align}

Note that this solution could solve by simply observing that \(\mathbf{\mathbf{b}^\trans \mathbf{A}}\) is actually a linear transformation from \(\mathbb{R}^{n}\) to \(\mathbb{R}\). Thus,
\begin{align}
    \dfrac{\partial \mathbf{b}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \left( \mathbf{b}^\trans \mathbf{A} \right)^{\trans} = \mathbf{A}^\trans \mathbf{b},
\end{align}
where the first equality comes from the \eqref{eq:lt-slution}.

\subsection{\(\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{b}}{\partial \mathbf{x}} = \mathbf{Ab}\)}
Let \(\mathbf{x} \in \mathbb{R}^{m}\), \(\mathbf{b} \in \mathbb{R}^{n}\) and \(\mathbf{A}\in \mathbb{R}^{m\times n}\), where neither \(\mathbf{b}\) nor \(\mathbf{A}\) depend on \(\mathbf{x}\). The quickest way to solve it is to note that \(\mathbf{x}^\trans \mathbf{A} \mathbf{b} =  \mathbf{b}^\trans \mathbf{A}^\trans \mathbf{x}\), which is the problem solved by the Section \ref{sec:bt-A-x}. Thus,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{b}}{\partial \mathbf{x}} = \dfrac{\partial \mathbf{b}^\trans \mathbf{A}^\trans \mathbf{x}}{\partial \mathbf{x}} = \left( \mathbf{b}^\trans \mathbf{A}^\trans \right)^{\trans} = \mathbf{A} \mathbf{b},
\end{align}
where the second equality comes from \eqref{eq:lt-slution}. Nevertheless, here is the step-by-step
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{b}}{\partial \mathbf{x}} &= \dfrac{\partial}{\partial \mathbf{x}} \left(
    \begin{bmatrix}
        x_{1} & x_{2} & \dots & x_{m}
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \dots & a_{mn} \\
    \end{bmatrix} \begin{bmatrix}
        b_{1} \\ b_{2} \\ \vdots \\ b_{n}
    \end{bmatrix} \right) \\
    &= \dfrac{\partial}{\partial \mathbf{x}} \left(
			\begin{bmatrix}
				\displaystyle \sum_{i = 1}^{m} a_{i1}x_{i} & 
				\displaystyle \sum_{i = 1}^{m} a_{i2}x_{i} & 
				\dots & 
				\displaystyle \sum_{i = 1}^{m} a_{in}x_{i}
			\end{bmatrix} \begin{bmatrix}
				b_{1} \\ b_{2} \\ \vdots \\ b_{n}
			\end{bmatrix} \right) \\
    &= \dfrac{\partial}{\partial \mathbf{x}} \left(
        \sum_{i = 1}^{m}\sum_{j = 1}^{n} a_{ij} b_{j} x_{i}
    \right) \\
    &= \sum_{i = 1}^{m}\sum_{j = 1}^{n} a_{ij} b_{j} \dfrac{\partial x_{i}}{\partial \mathbf{x}} \\
    &= \begin{bmatrix}
        \sum_{j = 1}^{n} a_{1j} b_{j} \\
        \sum_{j = 1}^{n} a_{2j} b_{j} \\
        \vdots \\
        \sum_{j = 1}^{n} a_{nj} b_{j} \\
    \end{bmatrix} \\
    &= \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \dots & a_{mn} \\
    \end{bmatrix}
    \begin{bmatrix}
        b_1 \\
        b_2 \\
        \vdots \\
        b_n
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{b}}{\partial \mathbf{x}} = \mathbf{Ab} \in \mathbb{R}^{m}}
\end{align}


\subsection{\(\dfrac{\partial \mathbf{y}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{v}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{v}} \mathbf{A}^{\trans} \mathbf{y} + \dfrac{\partial \mathbf{y}}{\partial \mathbf{v}} \mathbf{A} \mathbf{x}\)}
Let \(\mathbf{x} \in \mathbb{R}^{n}\), \(\mathbf{y} \in \mathbb{R}^{m}\), \(\mathbf{v} \in \mathbb{R}^{p}\), and \(\mathbf{A}\in \mathbb{R}^{m\times n}\), where \(\mathbf{x}\) and \(\mathbf{y}\) depend on \(\mathbf{v}\), but \(\mathbf{A}\) does not. Therefore,
\begin{align}
    \dfrac{\partial \mathbf{y}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{v}} &= \dfrac{\partial}{\partial \mathbf{v}} \left(
    \begin{bmatrix}
        y_{1} & y_{2} & \dots & y_{m}
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \dots & a_{mn} \\
    \end{bmatrix} \begin{bmatrix}
        x_{1} \\ x_{2} \\ \vdots \\ x_{n}
    \end{bmatrix} \right) \\
    &= \dfrac{\partial}{\partial \mathbf{v}} \left(
			\begin{bmatrix}
				\displaystyle \sum_{i = 1}^{m} a_{i1}y_{i} & 
				\displaystyle \sum_{i = 1}^{m} a_{i2}y_{i} & 
				\dots & 
				\displaystyle \sum_{i = 1}^{m} a_{in}y_{i}
			\end{bmatrix} \begin{bmatrix}
				x_{1} \\ x_{2} \\ \vdots \\ x_{n}
			\end{bmatrix} \right) \\
    &= \dfrac{\partial}{\partial \mathbf{v}} \left(
        \sum_{i = 1}^{m}\sum_{j = 1}^{n} a_{ij} x_{j} y_{i}
    \right) \\
    &= \sum_{i = 1}^{m}\sum_{j = 1}^{n} a_{ij} \dfrac{\partial y_{i}x_{j}}{\partial \mathbf{v}} %\\
    % &= \begin{bmatrix}
    %     \sum_{j = 1}^{n} a_{1j} b_{j} \\
    %     \sum_{j = 1}^{n} a_{2j} b_{j} \\
    %     \vdots \\
    %     \sum_{j = 1}^{n} a_{nj} b_{j} \\
    % \end{bmatrix} \\
    % &= \begin{bmatrix}
    %     a_{11} & a_{12} & \dots & a_{1n} \\
    %     a_{21} & a_{22} & \dots & a_{2n} \\
    %     \vdots & \vdots & \ddots & \vdots \\
    %     a_{m1} & a_{m2} & \dots & a_{mn} \\
    % \end{bmatrix}
    % \begin{bmatrix}
    %     b_1 \\
    %     b_2 \\
    %     \vdots \\
    %     b_n
    % \end{bmatrix}
\end{align}
Recalling that \((fg)' = f'g + g'f\), we have that
\begin{align}
    \dfrac{\partial \mathbf{y}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{v}} & = \sum_{i = 1}^{m}\sum_{j = 1}^{n} a_{ij} y_{i} \dfrac{\partial x_{j}}{\partial \mathbf{v}} + \sum_{i = 1}^{m}\sum_{j = 1}^{n} a_{ij} x_{j} \dfrac{\partial y_{i}}{\partial \mathbf{v}} \\
    & = \begin{bmatrix}
        \dfrac{\partial x_1}{\partial \mathbf{v}} & \dfrac{\partial x_2}{\partial \mathbf{v}} & \cdots & \dfrac{\partial x_n}{\partial \mathbf{v}}
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & a_{21} & \dots & a_{m1} \\
        a_{12} & a_{22} & \dots & a_{m2} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{1n} & a_{2n} & \dots & a_{mn} \\
    \end{bmatrix}
    \begin{bmatrix}
        y_1 \\
        y_2 \\
        \vdots \\
        y_m
    \end{bmatrix} + \\
    & \hspace{3ex} \begin{bmatrix}
        \dfrac{\partial y_1}{\partial \mathbf{v}} & \dfrac{\partial y_2}{\partial \mathbf{v}} & \cdots & \dfrac{\partial y_m}{\partial \mathbf{v}}
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \dots & a_{mn} \\
    \end{bmatrix}
    \begin{bmatrix}
        x_1 \\
        x_2 \\
        \vdots \\
        x_n
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{y}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{v}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{v}} \mathbf{A}^{\trans} \mathbf{y} + \dfrac{\partial \mathbf{y}}{\partial \mathbf{v}} \mathbf{A} \mathbf{x}}
\end{align}

Even though this problem is trickier, we can find the same solution in a clever way by preserving the Matrix Calculus notation and applying the chain rule. Note that \(\mathbf{y}^\trans \mathbf{A} \mathbf{x}\) depends on both \(\mathbf{x}\) and \(\mathbf{y}\) which, in turn, depend on \(\mathbf{v}\). Therefore (c.f. \eqref{eq:chain-multi-inter}),
\begin{align}
    \dfrac{\partial \mathbf{y}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{v}} & = \dfrac{\partial \mathbf{x}}{\partial \mathbf{v}} \dfrac{\partial \mathbf{y}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} + \dfrac{\partial \mathbf{y}}{\partial \mathbf{v}} \dfrac{\partial \mathbf{y}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{y}} \\
    & = \dfrac{\partial \mathbf{x}}{\partial \mathbf{v}} \mathbf{A}^{\trans} \mathbf{y} + \dfrac{\partial \mathbf{y}}{\partial \mathbf{v}} \mathbf{A} \mathbf{x}.
\end{align}

\subsection{\(\dfrac{\partial \textnormal{tr}\left(\mathbf{A} \mathbf{X}\right)}{\partial \mathbf{X}} = \mathbf{A}^\trans\)}

Let \(\mathbf{A}, \mathbf{X} \in \mathbb{R}^{n\times n}\), where \(\mathbf{A}\) does not depend on the elements in \(\mathbf{X}\).
\begin{align*}
    \dfrac{\partial \textnormal{tr}\left(\mathbf{A} \mathbf{X}\right)}{\partial \mathbf{X}} &= \dfrac{\partial}{\partial \mathbf{X}} \left( \tr{\begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & \dots & a_{nn} \\
    \end{bmatrix}
    \begin{bmatrix}
        x_{11} & x_{12} & \dots & x_{1n} \\
        x_{21} & x_{22} & \dots & x_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        x_{n1} & x_{n2} & \dots & x_{nn} \\
    \end{bmatrix} \right)} \\
    %
    &= \dfrac{\partial}{\partial \mathbf{X}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) \\
    %
    &= \begin{bmatrix}
        \displaystyle \dfrac{\partial}{\partial x_{11}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) & 
        \displaystyle \dfrac{\partial}{\partial x_{12}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) & 
        \dots & 
        \displaystyle \dfrac{\partial}{\partial x_{1n}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) \\
        \displaystyle \dfrac{\partial}{\partial x_{21}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) & 
        \displaystyle \dfrac{\partial}{\partial x_{22}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) & 
        \dots & 
        \displaystyle \dfrac{\partial}{\partial x_{2n}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) \\
        \vdots & \vdots & \ddots & \vdots \\
        \displaystyle \dfrac{\partial}{\partial x_{n1}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) & 
        \displaystyle \dfrac{\partial}{\partial x_{n2}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) & 
        \dots & 
        \displaystyle \dfrac{\partial}{\partial x_{nn}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) \\
    \end{bmatrix} \\
    &= \begin{bmatrix}
            a_{11} & a_{21} & \dots & a_{n1} \\
            a_{12} & a_{22} & \dots & a_{n2} \\
            \vdots & \vdots & \ddots & \vdots \\
            a_{1n} & a_{2n} & \dots & a_{nn} \\
        \end{bmatrix}
\end{align*}
\begin{align}
    \boxed{\dfrac{\partial \textnormal{tr}\left(\mathbf{A} \mathbf{X}\right)}{\partial \mathbf{X}} = \mathbf{A}^\trans}
\end{align}

I have no idea how to make this solution simpler.

\subsection{\(\dfrac{\partial \abs{\mathbf{X}}}{\partial \mathbf{X}} = \adj{\mathbf{X}}\)}
Let \(\mathbf{X} \in \mathbb{R}^{n\times n}\). Through Laplace expansion (cofactor expansion), we can rewrite the determinant of \(\mathbf{X}\) as the sum of the cofactors of any row or column, multiplied by its generating element, that is
\begin{align}
    \abs{\mathbf{X}} = \sum_{i = 1}^{n} x_{ki} \abs{\mathbf{C}_{ki}} = \sum_{i = 1}^{n} x_{ik} \abs{\mathbf{C}_{ik}} \,\,\,\,\,\, \forall \,\, k \in \left\{ 1, 2, ..., n \right\},
\end{align}
where \(\mathbf{C}_{ij}\) denotes the cofactor matrix of \(\mathbf{X}\) generated from element \(x_{ij}\). It is worth noting that the cofactor of \(\mathbf{C}_{ij}\) is independent of the value of any element \((i,j)\) in \(\mathbf{X}\). Therefore, it follows that
\begin{align}
    \dfrac{\partial \abs{\mathbf{X}}}{\partial \mathbf{X}} &= \dfrac{\partial}{\partial \mathbf{X}} \left( \sum_{i = 1}^{n} x_{ki} \abs{\mathbf{C}_{ki}} \right) \,\,\,\,\,\, \forall \,\, k \in \left\{ 1, 2, ..., n \right\} \\
    %
    & = \dfrac{\partial}{\partial \mathbf{X}} \left( \begin{bmatrix}
        \displaystyle \sum_{i = 1}^{n} x_{1i} \abs{\mathbf{C}_{1i}} & 
        \displaystyle \sum_{i = 1}^{n} x_{1i} \abs{\mathbf{C}_{1i}} & 
        \dots & 
        \displaystyle \sum_{i = 1}^{n} x_{1i} \abs{\mathbf{C}_{1i}} \\
        \displaystyle \sum_{i = 1}^{n} x_{2i} \abs{\mathbf{C}_{2i}} & 
        \displaystyle \sum_{i = 1}^{n} x_{2i} \abs{\mathbf{C}_{2i}} & 
        \dots & 
        \displaystyle \sum_{i = 1}^{n} x_{2i} \abs{\mathbf{C}_{2i}} \\
        \vdots & \vdots & \ddots & \vdots \\
        \displaystyle \sum_{i = 1}^{n} x_{ni} \abs{\mathbf{C}_{ni}} & 
        \displaystyle \sum_{i = 1}^{n} x_{ni} \abs{\mathbf{C}_{ni}} & 
        \dots & 
        \displaystyle \sum_{i = 1}^{n} x_{ni} \abs{\mathbf{C}_{ni}}
    \end{bmatrix} \right) \\
    %
    & = \begin{bmatrix}
        \displaystyle \dfrac{\partial}{\partial x_{11}} \left( \sum_{i = 1}^{n} x_{1i} \abs{\mathbf{C}_{1i}} \right) & 
        \displaystyle \dfrac{\partial}{\partial x_{12}} \left( \sum_{i = 1}^{n} x_{1i} \abs{\mathbf{C}_{1i}} \right) & 
        \dots & 
        \displaystyle \dfrac{\partial}{\partial x_{13}} \left( \sum_{i = 1}^{n} x_{1i} \abs{\mathbf{C}_{1i}} \right) \\
        \displaystyle \dfrac{\partial}{\partial x_{21}} \left( \sum_{i = 1}^{n} x_{2i} \abs{\mathbf{C}_{2i}} \right) & 
        \displaystyle \dfrac{\partial}{\partial x_{22}} \left( \sum_{i = 1}^{n} x_{2i} \abs{\mathbf{C}_{2i}} \right) & 
        \dots & 
        \displaystyle \dfrac{\partial}{\partial x_{33}} \left( \sum_{i = 1}^{n} x_{2i} \abs{\mathbf{C}_{2i}} \right) \\
        \vdots & \vdots & \ddots & \vdots \\
        \displaystyle \dfrac{\partial}{\partial x_{n1}} \left( \sum_{i = 1}^{n} x_{ni} \abs{\mathbf{C}_{ni}} \right) & 
        \displaystyle \dfrac{\partial}{\partial x_{n2}} \left( \sum_{i = 1}^{n} x_{ni} \abs{\mathbf{C}_{ni}} \right) & 
        \dots & 
        \displaystyle \dfrac{\partial}{\partial x_{n3}} \left( \sum_{i = 1}^{n} x_{ni} \abs{\mathbf{C}_{ni}} \right) \\
    \end{bmatrix} \\
    &= \begin{bmatrix}
        \abs{\mathbf{C}_{11}} & \abs{\mathbf{C}_{12}} & \dots & \abs{\mathbf{C}_{1n}} \\
        \abs{\mathbf{C}_{21}} & \abs{\mathbf{C}_{22}} & \dots & \abs{\mathbf{C}_{2n}} \\
        \vdots & \vdots & \ddots & \vdots \\
        \abs{\mathbf{C}_{n1}} & \abs{\mathbf{C}_{n2}} & \dots & \abs{\mathbf{C}_{nn}} \\
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \abs{\mathbf{X}}}{\partial \mathbf{X}} = \adj{\mathbf{X}}}
\end{align}

I have no idea how to make this solution simpler.

\subsection{\(\dfrac{\partial \mathbf{A}^{-1}}{\partial \alpha} = - {\left( \mathbf{A}^{-1} \right)}^{\trans} \dfrac{\partial \mathbf{A}}{\partial \alpha} {\left( \mathbf{A}^{-1} \right)}^{\trans}\)}
Let \(\mathbf{A}\in \mathbb{C}^{m\times n}\) and \(\alpha \in \mathbb{C}\). Remember that \(\mathbf{A}^{-1}\mathbf{A} = \mathbf{I}\). Differentiating both sides of this equation with respect to \(\alpha\), we get
\begin{align}
    \dfrac{\partial \mathbf{A}^{-1}\mathbf{A}}{\partial \alpha} & = \dfrac{\partial \mathbf{I}}{\partial \alpha} = \mathbf{0}_{m \times n},
\end{align}
where \(\mathbf{0}_{m \times n}\) is a zero matrix with dimension \(m \times n\). By applying the product rule of a matrix-matrix derivate, we get (c.f. \eqref{eq:matrix-matrix-product-rule})
\begin{align}
    \dfrac{\partial \mathbf{A}^{-1}\mathbf{A}}{\partial \alpha} =  \dfrac{\partial \mathbf{A}}{\partial \alpha} \left( \mathbf{A}^{-1} \right)^{\trans} + \mathbf{A}^{\trans} \dfrac{\partial \mathbf{A}^{-1}}{\partial \alpha} & = \mathbf{0}_{m \times n}
\end{align}
Using the property \(\left( \mathbf{A}^{\trans} \right)^{-1} = \left( \mathbf{A}^{-1} \right)^{\trans}\) and rearranging this expression, we get
\begin{align}
    \boxed{\dfrac{\partial \mathbf{A}^{-1}}{\partial \alpha} = - {\left( \mathbf{A}^{-1} \right)}^{\trans} \dfrac{\partial \mathbf{A}}{\partial \alpha} {\left( \mathbf{A}^{-1} \right)}^{\trans}}
\end{align}

\nocite{*}
\printbibliography

\end{document}