\section{Machine learning, optimization theory, and \newline statistical signal processing}
\subsection{Matrix Calculus (in denominator layout)}
PS: Estimated terms come with a hat above of it
\begin{xltabular}{\textwidth}{XX}
	\(\mathbf{g}, \nabla f, \dfrac{\partial f}{\partial \mathbf{w}}\)                                                                                                                      & Gradient descent vector, used in the steepest descent method, also called gradient descent method or deterministic gradient method.                                                                                                                                                                                                                                                                                                        \\ \hline
	\(\mathbf{g}\) if the gradient vector is \(\nabla f\) (or \(\hat{\mathbf{g}}\) if the gradient vector is \(\mathbf{g}\) \cite{haykinNeuralNetworksLearning2009})                       & Stochastic gradient descent (SGD) vector, i.e., instantaneous approximation of gradient descent vector                                                                                                                                                                                                                                                                                                                                     \\ \hline
	\(\mathbf{g}_{\mathbf{x}}, \nabla_{\mathbf{w}}f, \dfrac{\partial f}{\partial \mathbf{w}}\)                                                                                             & Gradient descent vector with respect \(\mathbf{w}\) \cite{bishopPatternRecognitionMachine2006}                                                                                                                                                                                                                                                                                                                                             \\ \hline
	\(\mathbf{J}, \dfrac{\partial \mathbf{y}^{\top}}{\partial \mathbf{x}}\), \(\nabla \mathbf{y}^\top\) \cite{haykinNeuralNetworksLearning2009}                                            & Jacobian matrix.                                                                                                                                                                                                                                                                                                                                                                                                                           \\ \hline
	\(\mathbf{H}\), \(\dfrac{\partial^2 f}{\partial \mathbf{w}^2}\), \(\nabla^2 f\) \cite{haykinNeuralNetworksLearning2009}, \(\nabla\nabla f\) \cite{bishopPatternRecognitionMachine2006} & Hessian matrix. The notation \(\nabla^2\) is sometimes used in Matrix Calculus to denote the second-order vector derivative. However, it must be avoided since, in Vector Calculus, \(\nabla^2\) also denotes the Laplacian operator which in turn may be scalar or vector Laplacian operator depending on whether \(f\) is scalar- or vector-valued, respectively. Some discussion about can be found in \cite{4693212, 1353761, 4560326} \\ \hline
\end{xltabular}

\subsection{Statistics: estimation and detection theory}
\begin{xltabular}{\textwidth}{XX}
    \(\mathbf{x}\) & output \\ \hline
    \(\mathbf{w}\) & Parameters \\ \hline
	\(p(\mathbf{x} \mid \mathbf{w})\), \(l(\mathbf{x} \mid \mathbf{w})\)\cite{leon-garciaProbabilityStatisticsRandom2007}                                                                                                                                                                                                & Likelihood function                                                                     \\ \hline
	\(\ln{p(\mathbf{x} \mid \mathbf{w})}\)                                                                                                                                                                                                                                                                           & Log-likelihood function                                                                                             \\ \hline
	\(\Lambda(\mathbf{x})\)\cite{leon-garciaProbabilityStatisticsRandom2007}, \(\frac{p(\mathbf{x} \mid H_1)}{p(\mathbf{x} \mid H_0)}\) \cite{leon-garciaProbabilityStatisticsRandom2007,kayFundamentalsStatisticalProcessing2009}, \(L(\mathbf{x})\) \cite{kayFundamentalsStatisticalProcessing2009,CharlesPES} & Likelihood ratio function (also called likelihood ratio test (LRT) \cite{kayFundamentalsStatisticalProcessing2009}) \\ \hline
	\(\Lambda_l(\mathbf{x})\), \(\mathcal{L}(\mathbf{x})\) \cite{CharlesPES}, \(l(\mathbf{x})\) \cite{kayFundamentalsStatisticalProcessing2009}                                                                                                                                                                  & Log-likelihood ratio (LLR \cite{kayFundamentalsStatisticalProcessing2009}) function                                 \\ \hline
	\(\hat{\rho}_{x,y}\)                                                                                                                                                                                                                                                                                         & Estimated Pearson correlation coefficient between \(x\) and \(y\)                                                   \\ \hline
	\(\mathcal{R}_k\)                                                                                                                                                                                                                                                                                            & \(k\)th Decision region                                                                                                    \\ \hline
    \(x(t)\overset{m.s.e}{=}y(t)\) & \(x(t)\) equals \(y(t)\) is the mean square error sense, that is \(\E{\abs{x(t) - y(t)}^2} = 0\) \\ \hline
    \(x(t) = \limean_{N\rightarrow\infty}{\sum_{i=1}^{N} x_i \phi_i(t)}\)\cite{vantreesDetectionEstimationModulation2004} & \(\lim_{N\rightarrow\infty} \E{\abs{x(t) - \sum_{i=1}^{N} x_i \phi_i(t)}^{2}} = 0 \) (l.i.m stands for ``limit in the mean''). It is analogous to the \(\overset{m.s.e}{=}\) notation, but denoting that they equal in the MSE sense only when \(N\rightarrow\infty\) 
\end{xltabular}


\subsection{Signals, (hyper)parameters, system performance, and criteria}
\begin{xltabular}{\textwidth}{XX}
	\(N\)                                                                                                                                                                          & Number of instances (or samples), i.e., \(n \in \left\{ 1, 2, \dots, N \right\}\)                                                                                                                                                                                                                                                      \\ \hline
	\(N_{\textnormal{trn}}\)                                                                                                                                                       & Number of instances in the training set, i.e., \(n \in \left\{ 1, 2, \dots, N_\textnormal{trn} \right\}\)                                                                                                                                                                                                                              \\ \hline
	\(N_{\textnormal{tst}}\)                                                                                                                                                       & Number of instances in the test set, i.e., \(n \in \left\{ 1, 2, \dots, N_\textnormal{tst} \right\}\)                                                                                                                                                                                                                                  \\ \hline
	\(N_{\textnormal{val}}\)                                                                                                                                                       & Number of instances in the validation set, i.e., \(n \in \left\{ 1, 2, \dots, N_\textnormal{val} \right\}\)                                                                                                                                                                                                                            \\ \hline
	\(N_e\)                                                                                                                                                                        & Number of epochs                                                                                                                                                                                                                                                                                                                       \\ \hline
	\(N_a\)                                                                                                                                                                        & Number os attributes                                                                                                                                                                                                                                                                                                                   \\ \hline
	\(K\) \cite{haykinNeuralNetworksLearning2009}                                                                                                                                  & Number of classes (which is the number of outputs in multiclass problems). Use \(k\) to iterate over it                                                                                                                                                                                                                                \\ \hline
	\(L\)                                                                                                                                                                          & Number of layers, i.e., the depth of the network. Use \(l\) to iterate over it                                                                                                                                                                                                                                                         \\ \hline
	\(M_l\), \(m_l\) \cite{haykinNeuralNetworksLearning2009}, \(J\) \cite{haykinNeuralNetworksLearning2009}                                                                        & Number of neurons at the \(l\)th layer. You might prefer \(J\) in the case of the single-layer perceptron (use \(j\) to iterate over it). If you want to iterate through it, a sensible variation of Haykin notation is \(M_l\), where \(m_l\) can be used as an iterator. \(m_0\) is the length of the input vector without the bias. \\ \hline
	\(\mathbf{x}(n), \mathbf{x}_n\)                                                                                                                                                & Input signal (in \(\mathbb{R}^{N_a + 1}\))                                                                                                                                                                                                                                                                                             \\ \hline
	\(x_0(n)\)                                                                                                                                                                     & Dummy input of the bais, which is usually \(\pm 1\). \(+1\) is prefered \cite{bishopPatternRecognitionMachine2006,haykinNeuralNetworksLearning2009}.                                                                                                                                                                                   \\ \hline
	\(\varphi(\cdot)\)\cite{haykinNeuralNetworksLearning2009}, \(h(\cdot)\)\cite{bishopPatternRecognitionMachine2006}                                                              & Activation function                                                                                                                                                                                                                                                                                                                    \\ \hline
	\(\varphi'(v_{m_l}^{(l)}(n))\)\cite{haykinNeuralNetworksLearning2009}, \(\frac{\partial y_{m_l}^{(l)}(n)}{\partial v_{m_l}^{(l)}(n)}\) \cite{haykinNeuralNetworksLearning2009} & Partial derivative of the activation function with respect to \(v_{m_l}^{(l)}(n)\) (\(m_l\) neuron at \(l\)th layer)                                                                                                                                                                                                                   \\ \hline
	\(y_{m_l}^{(l)}(n), \varphi \left( v_{m_l}^{(l)}(n) \right)\)\cite{haykinNeuralNetworksLearning2009}, \(\mathbf{t}_{m_l}^{(l)}(n)\)\cite{bishopPatternRecognitionMachine2006}  & Output signal (target) of the \(m_l\)th neuron at the \(l\)th layer                                                                                                                                                                                                                                                                    \\ \hline
	\(\mathbf{y}^{(l)}(n)\)                                                                                                                                                        & Output signal of the \(l\)th layer                                                                                                                                                                                                                                                                                                     \\ \hline
	\(\mathbf{y}(n)\), \(\mathbf{y}^{(L)}(n)\)                                                                                                                                     & Output of the neural network                                                                                                                                                                                                                                                                                                           \\ \hline
	\(\mathbf{d}(n), \mathbf{d}_n\)                                                                                                                                                & Desired label (in case of supervised learning). For multiclass classification, one-hot encoding is usually used. For binary (scalar) classification, however antipodal encoding, i.e., \(\left\{ -1, 1 \right\}\) is more recommended \cite{haykinNeuralNetworksLearning2009}.                                                         \\ \hline
	\(e_{m_l}(n)\)                                                                                                                                                                 & Error signal of the neuron \(m_l\) at the \(l\)th layer                                                                                                                                                                                                                                                                                \\ \hline
	\(\mathbf{e}(n)\), \(\mathbf{d}(n) - \mathbf{y}(n)\)                                                                                                                           & Error signal                                                                                                                                                                                                                                                                                                                           \\ \hline
	\(\mathbf{w}_{m_l}^{(l)}(n), \boldsymbol{\thetaup}_{m_l}^{(l)}(n)\)
	\(\begin{bmatrix}
		  w_{m_l,0}^{(l)}(n) & w_{m_l,1}^{(l)}(n) & \dots & w_{m_l,m_{l-1}}^{(l)}(n)
	  \end{bmatrix}\)                                                                                                    & Parameters, coefficients, or synaptic weights vector in the \(l\)th layer. In the case of Single Layer Perceptrons or adaptive filters, the superscript is omitted                                                                                                                                                                                                                               \\ \hline
	\(w_{m_l, 0}^{(l)}(n), b_{m_l}^{(l)}(n)\)                                                                                                                                      & Bias (the first term of the weight vector) of the \(l\)th layer                                                                                                                                                                                                                                                                        \\ \hline
	\(\mathbf{W}(n)\), \(\begin{bmatrix}
		                     \mathbf{w}(1) & \mathbf{w}(2) & \cdots & \mathbf{w}(N)
	                     \end{bmatrix}^\top\)                                                                                                                        & Matrix of the synaptic weights                                                                                                                                                                                                                                                                                                                       \\ \hline
	\(\tilde{\mathbf{W}}(n)\)                                                                                                                                                      & Matrix of the synaptic weights, but without the bias                                                                                                                                                                                                                                                                                   \\ \hline
	\(v_{m_l}^{(l)}(n)\), \(\mathbf{w}_{m_l}^{(l)\top}(n) \mathbf{y}_{m_{l-1}}^{(l-1)}(n)\)                                                                                        & Induced local field or activation potential. At the first layer \(\mathbf{y}_{m_{0}}^{(0)}(n) = \mathbf{x}(n)\) \cite{bishopPatternRecognitionMachine2006}                                                                                                                                                                             \\ \hline
	\(\mathbf{v}^{(l)}(n), \mathbf{W}^{(l)}(n) \mathbf{y}_{m_{l-1}}^{(l-1)}(n)\)                                                                                                   & Vector of the local fields at the \(l\)th layer                                                                                                                                                                                                                                                                                        \\ \hline
	\(\mathbf{w}^{\star}, \mathbf{w}_o, \boldsymbol{\thetaup}^{\star}, \boldsymbol{\thetaup}_o\)                                                                                   & Optimum value of the parameters, coefficients, or synaptic weights vector (\(\mathbf{w}^\ast\) is also used \cite{bishopPatternRecognitionMachine2006} but it is not recommended as it may be confused with the conjugation operator)                                                                                                  \\ \hline
	\(\delta_{m_l}^{(l)}(n)\), \(\frac{\partial\mathscr{E}(n)}{\partial v_{m_l}^{(l)} (n)}\)                                                                                       & Local gradient of the \(m_l\)th neuron of the \(l\)th layer.                                                                                                                                                                                                                                                                           \\ \hline
	\(\boldsymbol{\deltaup}^{(l)}(n)\)                                                                                                                                             & Vector of the local gradients of all neurons at the \(l\)th layer                                                                                                                                                                                                                                                                      \\ \hline
	\(\mathbf{X}, \begin{bmatrix}
		              \mathbf{x}(1) & \mathbf{x}(2) & \cdots & \mathbf{x}(N)
	              \end{bmatrix}\)                                                                                                                        & Data matrix \cite{haykinNeuralNetworksLearning2009}                                                                                                                                                                                                                                                                                                              \\ \hline
	\(\eta(n)\)                                                                                                                                                                    & Learning rate hyperparameter \cite{haykinNeuralNetworksLearning2009}                                                                                                                                                                                                                                                                   \\ \hline
	\(\mathscr{R}\)                                                                                                                                                                & Bayes risk or average risk \cite{haykinNeuralNetworksLearning2009}                                                                                                                                                                                                                                                                     \\ \hline
	\(c_{ij}, C_{ij}\)                                                                                                                                                             & Misclassification cost in deciding in favor of class \(\mathscr{C}_i\) (represented in the subspace \(\mathscr{H}_i\)) when the \(\mathscr{C}_j\) is the true class (used in Bayes classifiers/detectors) \cite{haykinNeuralNetworksLearning2009,CharlesPES}                                                                           \\ \hline
	\(\mathscr{C}_k\)\cite{haykinNeuralNetworksLearning2009}, \(\mathcal{C}_k\) \cite{bishopPatternRecognitionMachine2006}                                                         & \(k\)th class                                                                                                                                                                                                                                                                                                                          \\ \hline
	\(\mathscr{T}\) \cite{haykinNeuralNetworksLearning2009}, \(\mathbb{X}\) \cite{goodfellowDeepLearning2016}                                                                      & Training set, i.e., the set \(\left\{ \mathbf{x}(n), d(n) \right\}\) that is used in the training phase.                                                                                                                                                                                                                               \\ \hline
	\(\mathscr{H}_k\)                                                                                                                                                              & Subspace of the training vector belonging to the class \(\mathscr{C}_k\)                                                                                                                                                                                                                                                               \\ \hline
	\(\mathscr{H}\)                                                                                                                                                                & Complete space of the input vector, i.e., \(\mathscr{H}_1 \cup \mathscr{H}_2 \cup \cdots \mathscr{H}_K \)                                                                                                                                                                                                                              \\ \hline
	\(\mathscr{X}\) \cite{haykinNeuralNetworksLearning2009}                                                                                                                        & Set of all vectors in the training, batch, validation, or test dataset that were misclassified                                                                                                                                                                                                                                         \\ \hline
	\(\mathscr{E}(\mathbf{w}), \mathscr{E}(\mathbf{w}(n)), \mathscr{E}(n)\)                                                                                                        & Cost function or objective function (the way it is written depends on the purpose of the text)                                                                                                                                                                                                                                         \\ \hline
	\(J(\mathbf{w}), J(\mathbf{w}(n)), J(n)\)                                                                                                                                      & Alternative to the cost function                                                                                                                                                                                                                                                                                                       \\ \hline
	\(\Delta\mathscr{E}(\mathbf{w}(n)), \Delta\mathscr{E}(n), \mathscr{E}(\mathbf{w}(n+1)) - \mathscr{E}(\mathbf{w}(n))\)                                                          & Cost function or objective function (the way it is written depends on the purpose of the text)                                                                                                                                                                                                                                         \\ \hline
	\(\mathscr{E}_{\textnormal{av}}(\cdot)\)\cite{haykinNeuralNetworksLearning2009}                                                                                                & Error energy averaged over the training sample or the empirical risk                                                                                                                                                                                                                                                                   \\ \hline
	\(\rho\)                                                                                                                                                                       & Distance of the margin of separation between two classes (Support Vector Machine, SVM)                                                                                                                                                                                                                                                 \\ \hline
	\(g(\cdot)\)                                                                                                                                                                   & Discriminant function, i.e., \(g(\mathbf{w}^{\star}) = 0\)
\end{xltabular}