\section{Machine learning, optimization theory, and \newline statistical signal processing}
\subsection{Matrix Calculus (in denominator layout)}
\begin{xltabular}{\textwidth}{XX}
	\(\mathbf{g}, \nabla f, \dfrac{\partial f}{\partial \mathbf{w}}\)                                                       & Gradient descent vector, ``used'' in the steepest (or gradient) descent method                                                                                                                                                                                                                                                                                                                                                             \\ \hline
	\(\mathbf{g}_{\mathbf{x}}, \nabla_{\mathbf{w}}f, \dfrac{\partial f}{\partial \mathbf{w}}\)                              & Gradient descent vector with respect \(\mathbf{w}\) \cite{bishopPatternRecognitionMachine2006}                                                                                                                                                                                                                                                                                                                                             \\ \hline
	\(\mathbf{J}, \dfrac{\partial \mathbf{y}^{\top}}{\partial \mathbf{x}}\)                                                 & Jacobian matrix.                                                                                                                                                                                                                                                                                                                                                                                                                           \\ \hline
	\(\mathbf{H}\), \(\dfrac{\partial^2 f}{\partial \mathbf{w}^2}\), \(\nabla^2 f\) \cite{haykinNeuralNetworksLearning2009}, \(\nabla\nabla f\) \cite{bishopPatternRecognitionMachine2006} & Hessian matrix. The notation \(\nabla^2\) is sometimes used in Matrix Calculus to denote the second-order vector derivative. However, it must be avoided since, in Vector Calculus, \(\nabla^2\) also denotes the Laplacian operator which in turn may be scalar or vector Laplacian operator depending on whether \(f\) is scalar- or vector-valued, respectively. Some discussion about can be found in \cite{4693212, 1353761, 4560326} \\ \hline
\end{xltabular}
\subsection{Estimated terms}
\begin{xltabular}{\textwidth}{XX}
	\(\mathbf{g}\) (or \(\hat{\mathbf{g}}\) if the gradient vector is \(\mathbf{g}\))                             & Stochastic gradient descent (SGD), i.e., instantaneous approximation of gradient descent vector            \\ \hline
	\(\hat{x}(t)\) or \(\hat{x}[n]\)                                                                              & Estimate of \(x(t)\) or \(x[n]\)                                                                           \\ \hline
	\(\hat{\boldsymbol{\muup}}_x, \hat{\mathbf{m}}_x\)                                                            & Sample mean of \(x[n]\) or \(x(t)\)                                                                        \\ \hline
	\(\hat{\boldsymbol{\muup}}_\mathbf{x}, \hat{\mathbf{m}}_\mathbf{x}\)                                          & Sample mean vector of \(\mathbf{x}[n]\) or \(\mathbf{x}(t)\)                                               \\ \hline
	\(\hat{r}_x(\tau), \hat{R}_x(\tau)\)                                                                          & Estimated autocorrelation function of the signal \(x(t)\) or \(x[n]\) \cite{nossekAdaptiveArraySignal2015} \\ \hline
	\(\hat{S}_x(f), \hat{S}_x(j\omega)\)                                                                          & Estimated power spectral density (PSD) of \(x(t)\) in linear (\(f\)) or angular (\(\omega\)) frequency     \\ \hline
	\(\hat{\mathbf{R}}_\mathbf{x}\)                                                                               & Sample (auto)correlation matrix                                                                            \\ \hline
	\(\hat{r}_{x,d}(\tau), \hat{R}_{x,d}(\tau)\)                                                                  & Estimated cross-correlation between \(x[n]\) and \(d[n]\) or \(x(t)\) and \(d(t)\)                         \\ \hline
	\(\hat{S}_{x,y}(f), \hat{S}_{x,y}(j\omega)\)                                                                  & Estimated cross PSD of \(x(t)\) and \(y(t)\) in linear or angular (\(\omega\)) frequency                   \\ \hline
	\(\hat{\mathbf{R}}_\mathbf{xy}\)                                                                              & Sample cross-correlation matrix of \(\mathbf{R}_\mathbf{xy}\)                                              \\ \hline
	\(\hat{c}_x(\tau), \hat{C}_x(\tau)\)                                                                          & Estimated autocovariance function of the signal \(x(t)\) or \(x[n]\)                                       \\ \hline
	\(\hat{\mathbf{C}}_\mathbf{x}, \hat{\mathbf{K}}_\mathbf{x}, \hat{\boldsymbol{\Sigmaup}}_\mathbf{x}\)          & Sample (auto)covariance matrix                                                                             \\ \hline
	\(\hat{c}_{xy}(\tau), \hat{C}_{xy}(\tau)\)                                                                    & Estimated cross-covariance function of the signal \(x(t)\) or \(x[n]\)                                     \\ \hline
	\(\hat{\mathbf{C}}_{\mathbf{xy}}, \hat{\mathbf{K}}_{\mathbf{xy}}, \hat{\boldsymbol{\Sigmaup}}_{\mathbf{xy}}\) & Sample cross-covariance matrix                                                                             \\ \hline
	\(\hat{\mathbf{H}}\)                                                                                          & Estimate of the Hessian matrix
\end{xltabular}

\subsection{Signals, (hyper)parameters, system performance, and criteria}
\begin{xltabular}{\textwidth}{XX}
	\(N\)                                                                                                                                                                          & Number of instances (or samples), i.e., \(n \in \left\{ 1, 2, \dots, N \right\}\)                                                                                                                                                                                                                                                      \\ \hline
	\(N_{\textnormal{trn}}\)                                                                                                                                                       & Number of instances in the training set, i.e., \(n \in \left\{ 1, 2, \dots, N_\textnormal{trn} \right\}\)                                                                                                                                                                                                                              \\ \hline
	\(N_{\textnormal{tst}}\)                                                                                                                                                       & Number of instances in the test set, i.e., \(n \in \left\{ 1, 2, \dots, N_\textnormal{tst} \right\}\)                                                                                                                                                                                                                                  \\ \hline
	\(N_{\textnormal{val}}\)                                                                                                                                                       & Number of instances in the validation set, i.e., \(n \in \left\{ 1, 2, \dots, N_\textnormal{val} \right\}\)                                                                                                                                                                                                                            \\ \hline
	\(N_e\)                                                                                                                                                                        & Number of epochs                                                                                                                                                                                                                                                                                                                       \\ \hline
	\(N_a\)                                                                                                                                                                        & Number os attributes                                                                                                                                                                                                                                                                                                                   \\ \hline
	\(K\) \cite{haykinNeuralNetworksLearning2009}                                                                                                                               & Number of classes (which is the number of outputs in multiclass problems). Use \(k\) to iterate over it                                                                                                                                                                                                                                \\ \hline
	\(L\)                                                                                                                                                                          & Number of layers, i.e., the depth of the network. Use \(l\) to iterate over it                                                                                                                                                                                                                                                                                         \\ \hline
	\(M_l\), \(m_l\) \cite{haykinNeuralNetworksLearning2009}, \(J\) \cite{haykinNeuralNetworksLearning2009}                                                                  & Number of neurons at the \(l\)th layer. You might prefer \(J\) in the case of the single-layer perceptron (use \(j\) to iterate over it). If you want to iterate through it, a sensible variation of Haykin notation is \(M_l\), where \(m_l\) can be used as an iterator. \(m_0\) is the length of the input vector without the bias. \\ \hline
	\(\mathbf{x}(n), \mathbf{x}_n\)                                                                                                                                                & Input signal (in \(\mathbb{R}^{N_a + 1}\))                                                                                                                                                                                                                                                                                             \\ \hline
	\(x_0(n)\)                                                                                                                                                                     & Dummy input of the bais, which is usually \(\pm 1\). \(+1\) is prefered \cite{bishopPatternRecognitionMachine2006,haykinNeuralNetworksLearning2009}.                                                                                                                                                                                   \\ \hline
	\(\varphi(\cdot)\)\cite{haykinNeuralNetworksLearning2009}, \(h(\cdot)\)\cite{bishopPatternRecognitionMachine2006}                                                              & Activation function                                                                                                                                                                                                                                                                                                                    \\ \hline
	\(\varphi'(v_{m_l}^{(l)}(n))\)\cite{haykinNeuralNetworksLearning2009}, \(\frac{\partial y_{m_l}^{(l)}(n)}{\partial v_{m_l}^{(l)}(n)}\) \cite{haykinNeuralNetworksLearning2009} & Partial derivative of the activation function with respect to \(v_{m_l}^{(l)}(n)\) (\(m_l\) neuron at \(l\)th layer)                                                                                                                                                                                                                   \\ \hline
	\(y_{m_l}^{(l)}(n), \varphi \left( v_{m_l}^{(l)}(n) \right)\)                                                                                                                  & Output signal of the \(m_l\)th neuron at the \(l\)th layer                                                                                                                                                                                                                                                                             \\ \hline
	\(\mathbf{y}^{(l)}(n)\)                                                                                                                                                        & Output signal of the \(l\)th layer                                                                                                                                                                                                                                                                                                     \\ \hline
	\(\mathbf{y}(n)\), \(\mathbf{y}^{(L)}(n)\)                                                                                                                                     & Output of the neural network                                                                                                                                                                                                                                                                                                           \\ \hline
	\(\mathbf{d}(n), \mathbf{d}_n\)                                                                                                                                                & Desired label (in case of supervised learning). For multiclass classification, one-hot encoding is usually used. For binary (scalar) classification, however antipodal encoding, i.e., \(\left\{ -1, 1 \right\}\) is more recommended \cite{haykinNeuralNetworksLearning2009}.                                                         \\ \hline
	\(e_{m_l}(n)\)                                                                                                                                                                 & Error signal of the neuron \(m_l\) at the \(l\)th layer                                                                                                                                                                                                                                                                                \\ \hline
	\(\mathbf{e}(n)\), \(\mathbf{d}(n) - \mathbf{y}(n)\)                                                                                                                           & Error signal                                                                                                                                                                                                                                                                                                                           \\ \hline
	\(\mathbf{w}_{m_l}^{(l)}(n), \boldsymbol{\thetaup}_{m_l}^{(l)}(n)\)
	\(\begin{bmatrix}
		w_{m_l,0}^{(l)}(n) & w_{m_l,1}^{(l)}(n) & \dots & w_{m_l,m_{l-1}}^{(l)}(n)
	\end{bmatrix}\)                                                                                                    & Parameters, coefficients, or synaptic weights vector in the \(l\)th layer. In the case of Single Layer Perceptrons or adaptive filters, the superscript is omitted                                                                                                                                                                                                                                        \\ \hline
	\(w_{m_l, 0}^{(l)}(n), b_{m_l}^{(l)}(n)\)                                                                                                                                      & Bias (the first term of the weight vector) of the \(l\)th layer                                                                                                                                                                                                                                                                        \\ \hline
	\(\mathbf{W}(n)\), \(\begin{bmatrix}
		\mathbf{w}(1) & \mathbf{w}(2) & \cdots & \mathbf{w}(N)
	\end{bmatrix}^\top\)                                                                                                                        & Matrix of the synaptic weights                                                                                                                                                                                                                                                                                                                                \\ \hline
	\(\tilde{\mathbf{W}}(n)\)                                                                                                                                                      & Matrix of the synaptic weights, but without the bias                                                                                                                                                                                                                                                                                            \\ \hline
	\(v_{m_l}^{(l)}(n)\), \(\mathbf{w}_{m_l}^{(l)\top}(n) \mathbf{y}_{m_{l-1}}^{(l-1)}(n)\)                                                                                        & Induced local field or activation potential. At the first layer \(\mathbf{y}_{m_{0}}^{(0)}(n) = \mathbf{x}(n)\) \cite{bishopPatternRecognitionMachine2006}                                                                                                                                                                             \\ \hline
	\(\mathbf{v}^{(l)}(n), \mathbf{W}^{(l)}(n) \mathbf{y}_{m_{l-1}}^{(l-1)}(n)\)                                                                                                   & Vector of the local fields at the \(l\)th layer                                                                                                                                                                                                                                                                                        \\ \hline
	\(\mathbf{w}^{\star}, \mathbf{w}_o, \boldsymbol{\thetaup}^{\star}, \boldsymbol{\thetaup}_o\)                                                                                   & Optimum value of the parameters, coefficients, or synaptic weights vector (\(\mathbf{w}^\ast\) is also used \cite{bishopPatternRecognitionMachine2006} but it is not recommended as it may be confused with the conjugation operator)                                                                                                           \\ \hline
	\(\delta_{m_l}^{(l)}(n)\), \(\frac{\partial\mathscr{E}(n)}{\partial v_{m_l}^{(l)} (n)}\)                                                                                       & Local gradient of the \(m_l\)th neuron of the \(l\)th layer.                                                                                                                                                                                                                                                                           \\ \hline
	\(\boldsymbol{\deltaup}^{(l)}(n)\)                                                                                                                                             & Vector of the local gradients of all neurons at the \(l\)th layer                                                                                                                                                                                                                                                                      \\ \hline
	\(\mathbf{X}, \begin{bmatrix}
		\mathbf{x}(1) & \mathbf{x}(2) & \cdots & \mathbf{x}(N)
	\end{bmatrix}\)                                                                                                                        & Data matrix                                                                                                                                                                                                                                                                                                                                                      \\ \hline
	\(\eta(n)\)                                                                                                                                                                    & Learning rate hyperparameter \cite{haykinNeuralNetworksLearning2009}                                                                                                                                                                                                                                                                \\ \hline
	\(\mathscr{R}\)                                                                                                                                                                & Bayes risk or average risk \cite{haykinNeuralNetworksLearning2009}                                                                                                                                                                                                                                                                  \\ \hline
	\(c_{ij}, C_{ij}\)                                                                                                                                                             & Misclassification cost in deciding in favor of class \(\mathscr{C}_i\) (represented in the subspace \(\mathscr{H}_i\)) when the \(\mathscr{C}_j\) is the true class (used in Bayes classifiers/detectors) \cite{haykinNeuralNetworksLearning2009,CharlesPES}                                                                        \\ \hline
	\(\mathscr{C}_k\)                                                                                                                                                              & \(k\)th class \cite{haykinNeuralNetworksLearning2009}                                                                                                                                                                                                                                                                               \\ \hline
	\(\mathscr{T}\) \cite{haykinNeuralNetworksLearning2009}, \(\mathbb{X}\) \cite{goodfellowDeepLearning2016}                                                                                                                                                                & Training set, i.e., the set \(\left\{ \mathbf{x}(n), d(n) \right\}\) that is used in the training phase.                                                                                                                                                                                     \\ \hline
	\(\mathscr{H}_k\)                                                                                                                                                              & Subspace of the training vector belonging to the class \(\mathscr{C}_k\)                                                                                                                                                                                                                                                               \\ \hline
	\(\mathscr{H}\)                                                                                                                                                                & Complete space of the input vector, i.e., \(\mathscr{H}_1 \cup \mathscr{H}_2 \cup \cdots \mathscr{H}_K \)                                                                                                                                                                                                                              \\ \hline
	\(\mathscr{X}\) \cite{haykinNeuralNetworksLearning2009}                                                                                                                        & Set of all vectors in the training, batch, validation, or test dataset that was misclassified                                                                                                                                                                                                                                          \\ \hline
	\(\mathscr{E}(\mathbf{w}), \mathscr{E}(\mathbf{w}(n)), \mathscr{E}(n)\)                                                                                                        & Cost function or objective function (the way it is written depends on the purpose of the text)                                                                                                                                                                                                                                         \\ \hline
	\(J(\mathbf{w}), J(\mathbf{w}(n)), J(n)\)                                                                                                                                      & Alternative to the cost function                                                                                                                                                                                                                                                                                                       \\ \hline
	\(\Delta\mathscr{E}(\mathbf{w}(n)), \Delta\mathscr{E}(n), \mathscr{E}(\mathbf{w}(n+1)) - \mathscr{E}(\mathbf{w}(n))\)                                                          & Cost function or objective function (the way it is written depends on the purpose of the text)                                                                                                                                                                                                                                         \\ \hline
	\(\mathscr{E}_{\textnormal{av}}(\cdot)\)                                                                                                                                       & Error energy averaged over the training sample or the empirical risk  \cite{bishopPatternRecognitionMachine2006}                                                                                                                                                                                                                       \\ \hline
	\(\Lambda(\cdot)\)                                                                                                                                                             & Likelihood function                                                                                                                                                                                                                                                                                                                    \\ \hline
	\(\Lambda_l(\cdot)\)                                                                                                                                                           & Log-likelihood function                                                                                                                                                                                                                                                                                                                \\ \hline
	\(\hat{\rho}_{x,y}\)                                                                                                                                                           & Estimated Pearson correlation coefficient between \(x\) and \(y\)                                                                                                                                                                                                                                                                      \\ \hline
	\(\rho\)                                                                                                                                                                       & Distance of the margin of separation between two classes (Support Vector Machine, SVM)                                                                                                                                                                                                                                                 \\ \hline
	\(g(\cdot)\)                                                                                                                                                                   & Discriminant function, i.e., \(g(\mathbf{w}^{\star}) = 0\)
\end{xltabular}
